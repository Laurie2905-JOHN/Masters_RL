{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.preferences.preference_utils import (\n",
    "    get_child_data,\n",
    "    initialize_child_preference_data,\n",
    "    print_preference_difference_and_accuracy,\n",
    "    calculate_percent_of_known_ingredients_to_unknown,\n",
    "    plot_individual_child_known_percent,\n",
    "    plot_preference_and_sentiment_accuracies,\n",
    "    plot_utilities_and_mape,\n",
    "    plot_utilities_from_json,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Configure Matplotlib to use LaTeX for rendering\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",  # Use serif font in conjunction with LaTeX\n",
    "    \"text.latex.preamble\": r\"\\usepackage{times}\",\n",
    "})\n",
    "\n",
    "def plot_ingredient_data_histograms(ingredient_df, rotation=15, fontsize=12):\n",
    "    # Convert child_data to a DataFrame\n",
    "    df = pd.DataFrame(ingredient_df)\n",
    "\n",
    "    # Plotting histograms for each feature\n",
    "    features = ['Colour', 'Taste', 'Texture', 'Healthy']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(features):\n",
    "        if df[feature].dtype == 'object':\n",
    "            # For categorical data\n",
    "            df[feature].value_counts().plot(kind='bar', ax=axes[idx], color='skyblue')\n",
    "            axes[idx].set_ylabel('Count', fontsize=fontsize)\n",
    "            # Format the x-tick labels\n",
    "            formatted_labels = [label.replace('_', ' ').title() if isinstance(label, str) else label for label in df[feature].value_counts().index]\n",
    "            axes[idx].set_xticklabels(formatted_labels, rotation=rotation, fontsize=fontsize)\n",
    "        else:\n",
    "            # For numerical data\n",
    "            df[feature].plot(kind='hist', ax=axes[idx], bins=range(8, 13), color='skyblue')\n",
    "            axes[idx].set_ylabel('Frequency', fontsize=fontsize)\n",
    "            axes[idx].set_xticks(range(8, 13))\n",
    "            axes[idx].tick_params(axis='x', labelsize=fontsize)\n",
    "\n",
    "        # Capitalize and remove underscores in the feature name for xlabel\n",
    "        formatted_feature = feature.replace('_', ' ').title()\n",
    "        axes[idx].set_xlabel(formatted_feature, fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Count the number of 1s in specified groups\n",
    "    groups = ['Group A veg', 'Group A fruit', 'Group BC', 'Group D', 'Group E', 'Bread', 'Confectionary']\n",
    "    \n",
    "    group_rename = {\n",
    "        'Group A veg': 'Vegetables',\n",
    "        'Group A fruit': 'Fruits',\n",
    "        'Group BC': 'Protein',\n",
    "        'Group D': 'Carbs',\n",
    "        'Group E': 'Dairy',\n",
    "        'Bread': 'Bread',\n",
    "        'Confectionary': 'Confectionary'\n",
    "    }\n",
    "    \n",
    "    counts = {group: df[group].sum() for group in groups}\n",
    "\n",
    "    # Rename the groups for the plot\n",
    "    renamed_counts = {group_rename[group]: count for group, count in counts.items()}\n",
    "\n",
    "    # Plot the counts on a bar chart\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    ax.bar(renamed_counts.keys(), renamed_counts.values(), color='skyblue')\n",
    "    ax.set_xlabel('Groups', fontsize=fontsize)\n",
    "    ax.set_ylabel('Number of Ingredients in Group', fontsize=fontsize)\n",
    "    plt.xticks(rotation=15, fontsize=fontsize)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Configure Matplotlib to use LaTeX for rendering\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",  # Use serif font in conjunction with LaTeX\n",
    "    \"text.latex.preamble\": r\"\\usepackage{times}\",\n",
    "})\n",
    "\n",
    "    \n",
    "from utils.process_data import get_data\n",
    "ingredient_df = get_data(\"data.csv\")\n",
    "\n",
    "\n",
    "plot_ingredient_data_histograms(ingredient_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Configure Matplotlib to use LaTeX for rendering\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",  # Use serif font in conjunction with LaTeX\n",
    "    \"text.latex.preamble\": r\"\\usepackage{times}\",\n",
    "})\n",
    "\n",
    "def plot_child_data_histograms(child_data, rotation=0, fontsize=12):\n",
    "    # Convert child_data to a DataFrame\n",
    "    df = pd.DataFrame(child_data).T\n",
    "\n",
    "    # Plotting histograms for each feature\n",
    "    features = ['age', 'gender', 'health_consideration', 'favorite_cuisine']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(features):\n",
    "        if df[feature].dtype == 'object':\n",
    "            # For categorical data\n",
    "            df[feature].value_counts().plot(kind='bar', ax=axes[idx], color='skyblue')\n",
    "            axes[idx].set_ylabel('Count', fontsize=fontsize)\n",
    "            # Format the x-tick labels\n",
    "            formatted_labels = [label.replace('_', ' ').title() if isinstance(label, str) else label for label in df[feature].value_counts().index]\n",
    "            axes[idx].set_xticklabels(formatted_labels, rotation=rotation, fontsize=fontsize)\n",
    "        else:\n",
    "            # For numerical data\n",
    "            df[feature].plot(kind='hist', ax=axes[idx], bins=range(8, 13), color='skyblue')\n",
    "            axes[idx].set_ylabel('Frequency', fontsize=fontsize)\n",
    "            axes[idx].set_xticks(range(8, 13))\n",
    "            axes[idx].tick_params(axis='x', labelsize=fontsize)\n",
    "\n",
    "        # Capitalize and remove underscores in the feature name for xlabel\n",
    "        formatted_feature = feature.replace('_', ' ').title()\n",
    "        # axes[idx].set_title(f'Histogram of {formatted_feature}', fontsize=fontsize)\n",
    "        axes[idx].set_xlabel(formatted_feature, fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Configure Matplotlib to use LaTeX for rendering\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",  # Use serif font in conjunction with LaTeX\n",
    "    \"text.latex.preamble\": r\"\\usepackage{times}\",\n",
    "})\n",
    "\n",
    "    \n",
    "child_data = get_child_data()\n",
    "plot_child_data_histograms(child_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_df = get_data(\"data.csv\")\n",
    "child_data = get_child_data()\n",
    "data = initialize_child_preference_data(child_data=child_data, ingredient_df=ingredient_df, plot_graphs=True, child_key_plot='c2hild23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.preferences.sentiment_analysis import SentimentAnalyzer\n",
    "from models.preferences.menu_generators import RandomMenuGenerator\n",
    "from models.preferences.prediction import PreferenceModel\n",
    "from models.preferences.voting import IngredientNegotiator\n",
    "\n",
    "child_data = get_child_data()\n",
    "ingredient_df = get_data(\"data.csv\")\n",
    "true_child_preference_data = initialize_child_preference_data(child_data, ingredient_df, seed=None, plot_graphs=False, split = 0.5)\n",
    "\n",
    "# Set to zero for complete randomness\n",
    "probability_best = 0\n",
    "# Random is all equal and score is based on the score of the ingredient in terms of the negotiated list\n",
    "weight_type = \"random\"\n",
    "# weight_type = \"score\"\n",
    "menu_plan_length = 10\n",
    "seed = None\n",
    "\n",
    "# Complex weight function arguments\n",
    "complex_weight_func_args = {\n",
    "    'use_normalize_total_voting_weight': False,\n",
    "    'use_normalize_vote_categories': True,\n",
    "    'use_compensatory': True,\n",
    "    'use_feedback': True,\n",
    "    'use_fairness': True,\n",
    "    'target_gini': 0.15,\n",
    "}\n",
    "weight_function = 'simple'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the subplot grid\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()  # Flatten to easily iterate over the axes\n",
    "\n",
    "# Iterate over each model in the dictionary, except the 'perfect' model\n",
    "model_name_dict = {\n",
    "    'roberta': \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    'bertweet': \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
    "    'distilroberta': \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n",
    "    '5_star': \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    'perfect': \"perfect\",  # Skip this one\n",
    "    'TextBlob': \"TextBlob\",\n",
    "    'Vader': \"Vader\"\n",
    "}\n",
    "\n",
    "iterations = 50  # Number of iterations for each model\n",
    "index = 0  # Index for subplots\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    if model_name == \"perfect\":\n",
    "        continue  # Skip the 'perfect' model\n",
    "\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Initialize and run the preference model\n",
    "        predictor = PreferenceModel(\n",
    "            ingredient_df, child_data, true_child_preference_data, visualize_data=False, apply_SMOTE=True, file_path=None, seed=seed\n",
    "        )\n",
    "        updated_known_and_predicted_preferences = predictor.run_pipeline()\n",
    "\n",
    "        previous_feedback = {}\n",
    "        previous_utility = {}\n",
    "\n",
    "        # Initial negotiation of ingredients\n",
    "        negotiator = IngredientNegotiator(\n",
    "            seed, ingredient_df, updated_known_and_predicted_preferences, previous_feedback, previous_utility, complex_weight_func_args\n",
    "        )\n",
    "\n",
    "        negotiated_ingredients, unavailable_ingredients = negotiator.negotiate_ingredients(weight_function=weight_function)\n",
    "\n",
    "        # Calculate week and day\n",
    "        week = 1\n",
    "        day = 1\n",
    "\n",
    "        # Save negotiation results\n",
    "        negotiator.close(\"log_file.json\", week=week, day=day)\n",
    "\n",
    "        menu_generator = RandomMenuGenerator(menu_plan_length=10, weight_type='random', probability_best=probability_best, seed=None)\n",
    "\n",
    "        # Generate menu based on negotiated list\n",
    "        menu_plan = menu_generator.generate_menu(negotiated_ingredients, unavailable_ingredients, save_paths={'data': '', 'graphs': ''}, week=week, day=day)\n",
    "\n",
    "        # Sentiment analysis initiation, initially with true preference data and will adapt it to updated preferences from feedback\n",
    "        sentiment_analyzer = SentimentAnalyzer(\n",
    "            true_child_preference_data, menu_plan, model_name=model_name, seed=None\n",
    "        )\n",
    "\n",
    "        # Get updated preferences from feedback, the sentiment accuracy and feedback given\n",
    "        updated_known_unknown_preferences_with_feedback, sentiment_accuracy, feedback_given, true_labels, pred_labels = sentiment_analyzer.get_sentiment_and_update_data(plot_confusion_matrix=False)\n",
    "\n",
    "        # Remove None values from true_labels and corresponding pred_labels\n",
    "        filtered_true_labels = []\n",
    "        filtered_pred_labels = []\n",
    "\n",
    "        for true_label, pred_label in zip(true_labels, pred_labels):\n",
    "            if true_label is not None:  # Filter out None values\n",
    "                filtered_true_labels.append(true_label)\n",
    "                filtered_pred_labels.append(pred_label)\n",
    "\n",
    "        # Collect filtered labels from this iteration\n",
    "        all_true_labels.extend(filtered_true_labels)\n",
    "        all_pred_labels.extend(filtered_pred_labels)\n",
    "\n",
    "        # Assign the feedback given to the previous feedback for complex weight calculation\n",
    "        previous_feedback = feedback_given\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = accuracy_score(all_true_labels, all_pred_labels) * 100\n",
    "\n",
    "    # Generate and plot the confusion matrix\n",
    "    cm = confusion_matrix(all_true_labels, all_pred_labels, labels=['likes', 'neutral', 'dislikes'])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['likes', 'neutral', 'dislikes'])\n",
    "\n",
    "    # Plot on the current subplot\n",
    "    disp.plot(ax=axes[index], cmap=plt.cm.Blues, xticks_rotation=0)\n",
    "    axes[index].set_title(f'{model_name}, Accuracy: {accuracy:.1f}\\\\%', fontsize=14)\n",
    "    axes[index].set_xlabel('Predicted', fontsize=14)\n",
    "    axes[index].set_ylabel('True', fontsize=14)\n",
    "    axes[index].tick_params(axis='both', labelsize=12)\n",
    "\n",
    "    index += 1  # Move to the next subplot\n",
    "\n",
    "# Adjust layout spacing between subplots\n",
    "plt.subplots_adjust(hspace=0, wspace=-0.75)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 136 lines from the file. Loaded 135 ingredients.\n",
      "\n",
      "Overall Accuracy of Preferences: 0.961728\n",
      "Standard Deviation of Accuracies: 0.022798\n",
      "0.9246108949416343\n",
      "0.9617283950617282\n"
     ]
    }
   ],
   "source": [
    "from utils.process_data import get_data\n",
    "from models.preferences.preference_utils import get_child_data, initialize_child_preference_data, print_preference_difference_and_accuracy\n",
    "from models.preferences.prediction import PreferenceModel\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, make_scorer\n",
    "\n",
    "\n",
    "name ='Random Forest'\n",
    "\n",
    "\n",
    "child_feature_data = get_child_data()\n",
    "ingredient_df = get_data(\"data.csv\")\n",
    "initial_preference = initialize_child_preference_data(child_feature_data, ingredient_df, split = 0.5, seed=None, plot_graphs=False)\n",
    "\n",
    "predictor = PreferenceModel(\n",
    "    ingredient_df, child_feature_data, initial_preference, model_name=name, visualize_data=True, apply_SMOTE=True, file_path='preferences_visualization.png', seed=None\n",
    ")\n",
    "updated_preferences, true_labels, predicted_labels = predictor.run_pipeline()\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "accuracy_total, std = print_preference_difference_and_accuracy(initial_preference, updated_preferences, summary_only=True)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "print(accuracy_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 136 lines from the file. Loaded 135 ingredients.\n",
      "\n",
      "Overall Accuracy of Preferences: 0.886667\n",
      "Standard Deviation of Accuracies: 0.016469\n",
      "Iteration: 1, Model: Logistic Regression, SMOTE: True, Accuracy: 0.6372, Total Accuracy: 0.8867\n",
      "\n",
      "Overall Accuracy of Preferences: 0.976049\n",
      "Standard Deviation of Accuracies: 0.015142\n",
      "Iteration: 1, Model: Support Vector Machine, SMOTE: True, Accuracy: 0.9233, Total Accuracy: 0.9760\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from utils.process_data import get_data\n",
    "from models.preferences.preference_utils import get_child_data, initialize_child_preference_data, print_preference_difference_and_accuracy\n",
    "from models.preferences.prediction import PreferenceModel\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, make_scorer\n",
    "\n",
    "import os\n",
    "from utils.process_data import get_data\n",
    "from models.preferences.preference_utils import print_preference_difference_and_accuracy\n",
    "from models.preferences.preference_utils import get_child_data, initialize_child_preference_data\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver='liblinear', C=10, max_iter=10000, class_weight='balanced'),\n",
    "    \"Support Vector Machine\": SVC(C=1.0, kernel='rbf', probability=True, class_weight='balanced'),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, eval_metric='logloss', scale_pos_weight=1),  # XGBoost uses scale_pos_weight for class balancing\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=None, criterion='gini', class_weight='balanced'),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),  # No direct class_weight support\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50, learning_rate=1.0, algorithm='SAMME.R'),  # No direct class_weight support\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto'),  # No direct class_weight support\n",
    "    \"Decision Tree\": DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, class_weight='balanced'),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),  # No direct class_weight support\n",
    "    \"Stochastic Gradient Descent\": SGDClassifier(loss='hinge', alpha=0.0001, max_iter=2000, tol=1e-3, class_weight='balanced'),\n",
    "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=2000)  # No direct class_weight support\n",
    "}\n",
    "\n",
    "# Define scorers with `average='macro'` set only for the appropriate metrics\n",
    "scorers = {\n",
    "'precision_macro': make_scorer(precision_score, average='macro', zero_division=0),\n",
    "'recall_macro': make_scorer(recall_score, average='macro', zero_division=0),\n",
    "'f1_macro': make_scorer(f1_score, average='macro', zero_division=0),\n",
    "'accuracy': make_scorer(accuracy_score)  # No `average` parameter here\n",
    "}\n",
    "\n",
    "child_feature_data = get_child_data()\n",
    "ingredient_df = get_data(\"data.csv\")\n",
    "\n",
    "# Initial prediction of preferences\n",
    "file_path = os.path.join('', \"preferences_visualization.png\")\n",
    "\n",
    "\n",
    "\n",
    "# Assuming all imports and other necessary function definitions have been done as in the provided code.\n",
    "\n",
    "def evaluate_models():\n",
    "    # Evaluate models using cross-validation\n",
    "    results = []\n",
    "    \n",
    "    for iter in range(10):\n",
    "        initial_preference = initialize_child_preference_data(child_feature_data, ingredient_df, split = 0.7, seed=None, plot_graphs=False)\n",
    "        for apply_SMOTE in [True, False]:\n",
    "            for name, model in models.items():\n",
    "\n",
    "                predictor = PreferenceModel(\n",
    "                    ingredient_df, child_feature_data, initial_preference, model_name=name, visualize_data=False, apply_SMOTE=apply_SMOTE, file_path=file_path, seed=None\n",
    "                )\n",
    "                updated_preferences, true_labels, predicted_labels = predictor.run_pipeline()\n",
    "                accuracy_total, std = print_preference_difference_and_accuracy(initial_preference, updated_preferences, summary_only=True)\n",
    "                del updated_preferences\n",
    "                # Calculate accuracy\n",
    "                accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "                # Print the model, accuracy, and iteration\n",
    "                print(f\"Iteration: {iter + 1}, Model: {name}, SMOTE: {apply_SMOTE}, Accuracy: {accuracy:.4f}, Total Accuracy: {accuracy_total:.4f}\")\n",
    "\n",
    "                # Loop over each scorer\n",
    "                for scorer_name, scorer in scorers.items():\n",
    "                    # Calculate the score using the scorer\n",
    "                    if scorer_name in ['precision_macro', 'recall_macro', 'f1_macro']:\n",
    "                        score = scorer._score_func(true_labels, predicted_labels, average='macro')\n",
    "                    else:\n",
    "                        score = scorer._score_func(true_labels, predicted_labels)\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"Iteration\": iter,\n",
    "                        \"SMOTE\": apply_SMOTE,\n",
    "                        \"Model\": name,\n",
    "                        \"Metric\": scorer_name,\n",
    "                        \"Score\": score\n",
    "                    })\n",
    "\n",
    "    # Convert results to DataFrame for easy comparison\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def get_ranked_models(results_df):\n",
    "    # Rank models for each metric\n",
    "    ranked_results = results_df.copy()\n",
    "    ranked_results['Rank'] = ranked_results.groupby('Metric')['Score'].rank(ascending=False, method='min')\n",
    "\n",
    "    # Convert results to DataFrame for easy comparison\n",
    "    results_df_sorted = ranked_results.sort_values(by=['Metric', 'Rank'])\n",
    "\n",
    "    print(results_df_sorted)\n",
    "    return results_df_sorted\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "results_df = evaluate_models()\n",
    "sorted_results_df = get_ranked_models(results_df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 136 lines from the file. Loaded 135 ingredients.\n",
      "\n",
      "Overall Accuracy of Preferences: 0.886667\n",
      "Standard Deviation of Accuracies: 0.026718\n",
      "(0.8866666666666667, 0.026718057476846562)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.886667\n",
      "Standard Deviation of Accuracies: 0.026718\n",
      "Iteration: 1, Model: Logistic Regression, SMOTE: True, Accuracy: 0.3715, Total Accuracy: 0.8867\n",
      "\n",
      "Overall Accuracy of Preferences: 0.977778\n",
      "Standard Deviation of Accuracies: 0.015301\n",
      "(0.9777777777777779, 0.015300674947979917)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.977778\n",
      "Standard Deviation of Accuracies: 0.015301\n",
      "Iteration: 1, Model: Support Vector Machine, SMOTE: True, Accuracy: 0.2972, Total Accuracy: 0.9778\n",
      "\n",
      "Overall Accuracy of Preferences: 0.951111\n",
      "Standard Deviation of Accuracies: 0.024834\n",
      "(0.9511111111111112, 0.02483415498405956)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.951111\n",
      "Standard Deviation of Accuracies: 0.024834\n",
      "Iteration: 1, Model: XGBoost, SMOTE: True, Accuracy: 0.3526, Total Accuracy: 0.9511\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980741\n",
      "Standard Deviation of Accuracies: 0.012773\n",
      "(0.9807407407407406, 0.012772856450743273)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980741\n",
      "Standard Deviation of Accuracies: 0.012773\n",
      "Iteration: 1, Model: Random Forest, SMOTE: True, Accuracy: 0.3020, Total Accuracy: 0.9807\n",
      "\n",
      "Overall Accuracy of Preferences: 0.960988\n",
      "Standard Deviation of Accuracies: 0.023341\n",
      "(0.9609876543209875, 0.023340844045622654)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.960988\n",
      "Standard Deviation of Accuracies: 0.023341\n",
      "Iteration: 1, Model: Gradient Boosting, SMOTE: True, Accuracy: 0.3352, Total Accuracy: 0.9610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.880741\n",
      "Standard Deviation of Accuracies: 0.021499\n",
      "(0.8807407407407408, 0.021498503260286645)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.880741\n",
      "Standard Deviation of Accuracies: 0.021499\n",
      "Iteration: 1, Model: AdaBoost, SMOTE: True, Accuracy: 0.3715, Total Accuracy: 0.8807\n",
      "\n",
      "Overall Accuracy of Preferences: 0.958765\n",
      "Standard Deviation of Accuracies: 0.021270\n",
      "(0.9587654320987654, 0.021270426226451117)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.958765\n",
      "Standard Deviation of Accuracies: 0.021270\n",
      "Iteration: 1, Model: K-Nearest Neighbors, SMOTE: True, Accuracy: 0.3296, Total Accuracy: 0.9588\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980741\n",
      "Standard Deviation of Accuracies: 0.015253\n",
      "(0.9807407407407409, 0.01525278539405481)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980741\n",
      "Standard Deviation of Accuracies: 0.015253\n",
      "Iteration: 1, Model: Decision Tree, SMOTE: True, Accuracy: 0.3004, Total Accuracy: 0.9807\n",
      "\n",
      "Overall Accuracy of Preferences: 0.850617\n",
      "Standard Deviation of Accuracies: 0.041291\n",
      "(0.8506172839506171, 0.04129071360519105)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.850617\n",
      "Standard Deviation of Accuracies: 0.041291\n",
      "Iteration: 1, Model: Gaussian Naive Bayes, SMOTE: True, Accuracy: 0.4332, Total Accuracy: 0.8506\n",
      "\n",
      "Overall Accuracy of Preferences: 0.889877\n",
      "Standard Deviation of Accuracies: 0.029613\n",
      "(0.8898765432098765, 0.029613164149264095)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.889877\n",
      "Standard Deviation of Accuracies: 0.029613\n",
      "Iteration: 1, Model: Stochastic Gradient Descent, SMOTE: True, Accuracy: 0.3747, Total Accuracy: 0.8899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.984198\n",
      "Standard Deviation of Accuracies: 0.012356\n",
      "(0.9841975308641976, 0.012355551608095618)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.984198\n",
      "Standard Deviation of Accuracies: 0.012356\n",
      "Iteration: 1, Model: MLP Classifier, SMOTE: True, Accuracy: 0.2972, Total Accuracy: 0.9842\n",
      "\n",
      "Overall Accuracy of Preferences: 0.889136\n",
      "Standard Deviation of Accuracies: 0.021167\n",
      "(0.8891358024691358, 0.021166989797676818)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.889136\n",
      "Standard Deviation of Accuracies: 0.021167\n",
      "Iteration: 1, Model: Logistic Regression, SMOTE: False, Accuracy: 0.3644, Total Accuracy: 0.8891\n",
      "\n",
      "Overall Accuracy of Preferences: 0.979012\n",
      "Standard Deviation of Accuracies: 0.012984\n",
      "(0.9790123456790123, 0.012983522790256698)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.979012\n",
      "Standard Deviation of Accuracies: 0.012984\n",
      "Iteration: 1, Model: Support Vector Machine, SMOTE: False, Accuracy: 0.2972, Total Accuracy: 0.9790\n",
      "\n",
      "Overall Accuracy of Preferences: 0.958272\n",
      "Standard Deviation of Accuracies: 0.020374\n",
      "(0.9582716049382718, 0.02037448518111233)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.958272\n",
      "Standard Deviation of Accuracies: 0.020374\n",
      "Iteration: 1, Model: XGBoost, SMOTE: False, Accuracy: 0.3368, Total Accuracy: 0.9583\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980741\n",
      "Standard Deviation of Accuracies: 0.012483\n",
      "(0.9807407407407408, 0.012483184849150161)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980741\n",
      "Standard Deviation of Accuracies: 0.012483\n",
      "Iteration: 1, Model: Random Forest, SMOTE: False, Accuracy: 0.3012, Total Accuracy: 0.9807\n",
      "\n",
      "Overall Accuracy of Preferences: 0.964938\n",
      "Standard Deviation of Accuracies: 0.021633\n",
      "(0.9649382716049383, 0.021632786285243948)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.964938\n",
      "Standard Deviation of Accuracies: 0.021633\n",
      "Iteration: 1, Model: Gradient Boosting, SMOTE: False, Accuracy: 0.3281, Total Accuracy: 0.9649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.898025\n",
      "Standard Deviation of Accuracies: 0.024469\n",
      "(0.8980246913580248, 0.024469372499509204)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.898025\n",
      "Standard Deviation of Accuracies: 0.024469\n",
      "Iteration: 1, Model: AdaBoost, SMOTE: False, Accuracy: 0.3700, Total Accuracy: 0.8980\n",
      "\n",
      "Overall Accuracy of Preferences: 0.960000\n",
      "Standard Deviation of Accuracies: 0.021520\n",
      "(0.9600000000000002, 0.021519761550124358)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.960000\n",
      "Standard Deviation of Accuracies: 0.021520\n",
      "Iteration: 1, Model: K-Nearest Neighbors, SMOTE: False, Accuracy: 0.3249, Total Accuracy: 0.9600\n",
      "\n",
      "Overall Accuracy of Preferences: 0.984198\n",
      "Standard Deviation of Accuracies: 0.011272\n",
      "(0.9841975308641975, 0.011271814528902059)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.984198\n",
      "Standard Deviation of Accuracies: 0.011272\n",
      "Iteration: 1, Model: Decision Tree, SMOTE: False, Accuracy: 0.2996, Total Accuracy: 0.9842\n",
      "\n",
      "Overall Accuracy of Preferences: 0.852840\n",
      "Standard Deviation of Accuracies: 0.041584\n",
      "(0.8528395061728395, 0.04158423487517762)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.852840\n",
      "Standard Deviation of Accuracies: 0.041584\n",
      "Iteration: 1, Model: Gaussian Naive Bayes, SMOTE: False, Accuracy: 0.4269, Total Accuracy: 0.8528\n",
      "\n",
      "Overall Accuracy of Preferences: 0.891358\n",
      "Standard Deviation of Accuracies: 0.022167\n",
      "(0.8913580246913582, 0.022167284629593515)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.891358\n",
      "Standard Deviation of Accuracies: 0.022167\n",
      "Iteration: 1, Model: Stochastic Gradient Descent, SMOTE: False, Accuracy: 0.3708, Total Accuracy: 0.8914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.984444\n",
      "Standard Deviation of Accuracies: 0.012149\n",
      "(0.9844444444444443, 0.012149051456930906)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.984444\n",
      "Standard Deviation of Accuracies: 0.012149\n",
      "Iteration: 1, Model: MLP Classifier, SMOTE: False, Accuracy: 0.2964, Total Accuracy: 0.9844\n",
      "\n",
      "Overall Accuracy of Preferences: 0.893827\n",
      "Standard Deviation of Accuracies: 0.024888\n",
      "(0.8938271604938273, 0.024888105023909434)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.893827\n",
      "Standard Deviation of Accuracies: 0.024888\n",
      "Iteration: 2, Model: Logistic Regression, SMOTE: True, Accuracy: 0.3700, Total Accuracy: 0.8938\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980247\n",
      "Standard Deviation of Accuracies: 0.011042\n",
      "(0.980246913580247, 0.01104231099999896)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980247\n",
      "Standard Deviation of Accuracies: 0.011042\n",
      "Iteration: 2, Model: Support Vector Machine, SMOTE: True, Accuracy: 0.2957, Total Accuracy: 0.9802\n",
      "\n",
      "Overall Accuracy of Preferences: 0.948889\n",
      "Standard Deviation of Accuracies: 0.021499\n",
      "(0.948888888888889, 0.021498503260286645)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.948889\n",
      "Standard Deviation of Accuracies: 0.021499\n",
      "Iteration: 2, Model: XGBoost, SMOTE: True, Accuracy: 0.3470, Total Accuracy: 0.9489\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980988\n",
      "Standard Deviation of Accuracies: 0.011895\n",
      "(0.9809876543209877, 0.011895495773279917)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980988\n",
      "Standard Deviation of Accuracies: 0.011895\n",
      "Iteration: 2, Model: Random Forest, SMOTE: True, Accuracy: 0.2972, Total Accuracy: 0.9810\n",
      "\n",
      "Overall Accuracy of Preferences: 0.965926\n",
      "Standard Deviation of Accuracies: 0.016184\n",
      "(0.9659259259259259, 0.016183673687085356)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.965926\n",
      "Standard Deviation of Accuracies: 0.016184\n",
      "Iteration: 2, Model: Gradient Boosting, SMOTE: True, Accuracy: 0.3296, Total Accuracy: 0.9659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.884691\n",
      "Standard Deviation of Accuracies: 0.020836\n",
      "(0.8846913580246915, 0.02083605377757014)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.884691\n",
      "Standard Deviation of Accuracies: 0.020836\n",
      "Iteration: 2, Model: AdaBoost, SMOTE: True, Accuracy: 0.3597, Total Accuracy: 0.8847\n",
      "\n",
      "Overall Accuracy of Preferences: 0.957037\n",
      "Standard Deviation of Accuracies: 0.022500\n",
      "(0.9570370370370369, 0.022500317530655824)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.957037\n",
      "Standard Deviation of Accuracies: 0.022500\n",
      "Iteration: 2, Model: K-Nearest Neighbors, SMOTE: True, Accuracy: 0.3217, Total Accuracy: 0.9570\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980247\n",
      "Standard Deviation of Accuracies: 0.012590\n",
      "(0.980246913580247, 0.012590171638500697)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980247\n",
      "Standard Deviation of Accuracies: 0.012590\n",
      "Iteration: 2, Model: Decision Tree, SMOTE: True, Accuracy: 0.2941, Total Accuracy: 0.9802\n",
      "\n",
      "Overall Accuracy of Preferences: 0.830370\n",
      "Standard Deviation of Accuracies: 0.027407\n",
      "(0.8303703703703703, 0.027407407407407415)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.830370\n",
      "Standard Deviation of Accuracies: 0.027407\n",
      "Iteration: 2, Model: Gaussian Naive Bayes, SMOTE: True, Accuracy: 0.3636, Total Accuracy: 0.8304\n",
      "\n",
      "Overall Accuracy of Preferences: 0.898025\n",
      "Standard Deviation of Accuracies: 0.018413\n",
      "(0.8980246913580247, 0.018412867345681136)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.898025\n",
      "Standard Deviation of Accuracies: 0.018413\n",
      "Iteration: 2, Model: Stochastic Gradient Descent, SMOTE: True, Accuracy: 0.3431, Total Accuracy: 0.8980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.983457\n",
      "Standard Deviation of Accuracies: 0.008485\n",
      "(0.9834567901234568, 0.008485353223426965)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.983457\n",
      "Standard Deviation of Accuracies: 0.008485\n",
      "Iteration: 2, Model: MLP Classifier, SMOTE: True, Accuracy: 0.2909, Total Accuracy: 0.9835\n",
      "\n",
      "Overall Accuracy of Preferences: 0.889630\n",
      "Standard Deviation of Accuracies: 0.024816\n",
      "(0.8896296296296299, 0.024815736117041437)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.889630\n",
      "Standard Deviation of Accuracies: 0.024816\n",
      "Iteration: 2, Model: Logistic Regression, SMOTE: False, Accuracy: 0.3621, Total Accuracy: 0.8896\n",
      "\n",
      "Overall Accuracy of Preferences: 0.975802\n",
      "Standard Deviation of Accuracies: 0.017730\n",
      "(0.9758024691358024, 0.017729701798534555)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.975802\n",
      "Standard Deviation of Accuracies: 0.017730\n",
      "Iteration: 2, Model: Support Vector Machine, SMOTE: False, Accuracy: 0.3059, Total Accuracy: 0.9758\n",
      "\n",
      "Overall Accuracy of Preferences: 0.956543\n",
      "Standard Deviation of Accuracies: 0.021446\n",
      "(0.9565432098765433, 0.021445976049983655)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.956543\n",
      "Standard Deviation of Accuracies: 0.021446\n",
      "Iteration: 2, Model: XGBoost, SMOTE: False, Accuracy: 0.3383, Total Accuracy: 0.9565\n",
      "\n",
      "Overall Accuracy of Preferences: 0.979753\n",
      "Standard Deviation of Accuracies: 0.012820\n",
      "(0.9797530864197531, 0.012820498751355238)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.979753\n",
      "Standard Deviation of Accuracies: 0.012820\n",
      "Iteration: 2, Model: Random Forest, SMOTE: False, Accuracy: 0.2996, Total Accuracy: 0.9798\n",
      "\n",
      "Overall Accuracy of Preferences: 0.962222\n",
      "Standard Deviation of Accuracies: 0.020630\n",
      "(0.9622222222222222, 0.020630216983016588)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.962222\n",
      "Standard Deviation of Accuracies: 0.020630\n",
      "Iteration: 2, Model: Gradient Boosting, SMOTE: False, Accuracy: 0.3233, Total Accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.885679\n",
      "Standard Deviation of Accuracies: 0.025133\n",
      "(0.885679012345679, 0.025133085787847974)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.885679\n",
      "Standard Deviation of Accuracies: 0.025133\n",
      "Iteration: 2, Model: AdaBoost, SMOTE: False, Accuracy: 0.3542, Total Accuracy: 0.8857\n",
      "\n",
      "Overall Accuracy of Preferences: 0.953333\n",
      "Standard Deviation of Accuracies: 0.024429\n",
      "(0.9533333333333334, 0.024429475401739727)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.953333\n",
      "Standard Deviation of Accuracies: 0.024429\n",
      "Iteration: 2, Model: K-Nearest Neighbors, SMOTE: False, Accuracy: 0.3178, Total Accuracy: 0.9533\n",
      "\n",
      "Overall Accuracy of Preferences: 0.982469\n",
      "Standard Deviation of Accuracies: 0.010207\n",
      "(0.9824691358024692, 0.010207420547454034)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.982469\n",
      "Standard Deviation of Accuracies: 0.010207\n",
      "Iteration: 2, Model: Decision Tree, SMOTE: False, Accuracy: 0.2972, Total Accuracy: 0.9825\n",
      "\n",
      "Overall Accuracy of Preferences: 0.834815\n",
      "Standard Deviation of Accuracies: 0.029015\n",
      "(0.8348148148148148, 0.029015234949381512)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.834815\n",
      "Standard Deviation of Accuracies: 0.029015\n",
      "Iteration: 2, Model: Gaussian Naive Bayes, SMOTE: False, Accuracy: 0.3747, Total Accuracy: 0.8348\n",
      "\n",
      "Overall Accuracy of Preferences: 0.889383\n",
      "Standard Deviation of Accuracies: 0.022705\n",
      "(0.8893827160493828, 0.022705311471748427)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.889383\n",
      "Standard Deviation of Accuracies: 0.022705\n",
      "Iteration: 2, Model: Stochastic Gradient Descent, SMOTE: False, Accuracy: 0.3708, Total Accuracy: 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.983951\n",
      "Standard Deviation of Accuracies: 0.007670\n",
      "(0.9839506172839506, 0.007670234354078563)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.983951\n",
      "Standard Deviation of Accuracies: 0.007670\n",
      "Iteration: 2, Model: MLP Classifier, SMOTE: False, Accuracy: 0.2988, Total Accuracy: 0.9840\n",
      "\n",
      "Overall Accuracy of Preferences: 0.881975\n",
      "Standard Deviation of Accuracies: 0.023883\n",
      "(0.8819753086419754, 0.023883066250555536)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.881975\n",
      "Standard Deviation of Accuracies: 0.023883\n",
      "Iteration: 3, Model: Logistic Regression, SMOTE: True, Accuracy: 0.3628, Total Accuracy: 0.8820\n",
      "\n",
      "Overall Accuracy of Preferences: 0.973827\n",
      "Standard Deviation of Accuracies: 0.014406\n",
      "(0.9738271604938272, 0.014405878662533154)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.973827\n",
      "Standard Deviation of Accuracies: 0.014406\n",
      "Iteration: 3, Model: Support Vector Machine, SMOTE: True, Accuracy: 0.3146, Total Accuracy: 0.9738\n",
      "\n",
      "Overall Accuracy of Preferences: 0.940000\n",
      "Standard Deviation of Accuracies: 0.020807\n",
      "(0.9400000000000002, 0.020806773192871317)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.940000\n",
      "Standard Deviation of Accuracies: 0.020807\n",
      "Iteration: 3, Model: XGBoost, SMOTE: True, Accuracy: 0.3581, Total Accuracy: 0.9400\n",
      "\n",
      "Overall Accuracy of Preferences: 0.978272\n",
      "Standard Deviation of Accuracies: 0.012820\n",
      "(0.9782716049382717, 0.01282049875135523)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.978272\n",
      "Standard Deviation of Accuracies: 0.012820\n",
      "Iteration: 3, Model: Random Forest, SMOTE: True, Accuracy: 0.3051, Total Accuracy: 0.9783\n",
      "\n",
      "Overall Accuracy of Preferences: 0.950864\n",
      "Standard Deviation of Accuracies: 0.016283\n",
      "(0.9508641975308643, 0.016283197129676578)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.950864\n",
      "Standard Deviation of Accuracies: 0.016283\n",
      "Iteration: 3, Model: Gradient Boosting, SMOTE: True, Accuracy: 0.3431, Total Accuracy: 0.9509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.873333\n",
      "Standard Deviation of Accuracies: 0.023295\n",
      "(0.8733333333333333, 0.023295089175579826)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.873333\n",
      "Standard Deviation of Accuracies: 0.023295\n",
      "Iteration: 3, Model: AdaBoost, SMOTE: True, Accuracy: 0.3621, Total Accuracy: 0.8733\n",
      "\n",
      "Overall Accuracy of Preferences: 0.955062\n",
      "Standard Deviation of Accuracies: 0.021968\n",
      "(0.9550617283950618, 0.021968371772380462)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.955062\n",
      "Standard Deviation of Accuracies: 0.021968\n",
      "Iteration: 3, Model: K-Nearest Neighbors, SMOTE: True, Accuracy: 0.3241, Total Accuracy: 0.9551\n",
      "\n",
      "Overall Accuracy of Preferences: 0.978765\n",
      "Standard Deviation of Accuracies: 0.012207\n",
      "(0.9787654320987654, 0.012206624290674834)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.978765\n",
      "Standard Deviation of Accuracies: 0.012207\n",
      "Iteration: 3, Model: Decision Tree, SMOTE: True, Accuracy: 0.3075, Total Accuracy: 0.9788\n",
      "\n",
      "Overall Accuracy of Preferences: 0.846420\n",
      "Standard Deviation of Accuracies: 0.035107\n",
      "(0.8464197530864198, 0.03510690881417525)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.846420\n",
      "Standard Deviation of Accuracies: 0.035107\n",
      "Iteration: 3, Model: Gaussian Naive Bayes, SMOTE: True, Accuracy: 0.4182, Total Accuracy: 0.8464\n",
      "\n",
      "Overall Accuracy of Preferences: 0.890123\n",
      "Standard Deviation of Accuracies: 0.026986\n",
      "(0.8901234567901235, 0.026985971280243347)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.890123\n",
      "Standard Deviation of Accuracies: 0.026986\n",
      "Iteration: 3, Model: Stochastic Gradient Descent, SMOTE: True, Accuracy: 0.3700, Total Accuracy: 0.8901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.982222\n",
      "Standard Deviation of Accuracies: 0.011728\n",
      "(0.9822222222222223, 0.011727745271148567)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.982222\n",
      "Standard Deviation of Accuracies: 0.011728\n",
      "Iteration: 3, Model: MLP Classifier, SMOTE: True, Accuracy: 0.3028, Total Accuracy: 0.9822\n",
      "\n",
      "Overall Accuracy of Preferences: 0.890617\n",
      "Standard Deviation of Accuracies: 0.021184\n",
      "(0.8906172839506173, 0.02118426427535964)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.890617\n",
      "Standard Deviation of Accuracies: 0.021184\n",
      "Iteration: 3, Model: Logistic Regression, SMOTE: False, Accuracy: 0.3486, Total Accuracy: 0.8906\n",
      "\n",
      "Overall Accuracy of Preferences: 0.974074\n",
      "Standard Deviation of Accuracies: 0.013216\n",
      "(0.9740740740740741, 0.01321622092945445)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.974074\n",
      "Standard Deviation of Accuracies: 0.013216\n",
      "Iteration: 3, Model: Support Vector Machine, SMOTE: False, Accuracy: 0.3012, Total Accuracy: 0.9741\n",
      "\n",
      "Overall Accuracy of Preferences: 0.948148\n",
      "Standard Deviation of Accuracies: 0.018543\n",
      "(0.9481481481481482, 0.018543193437549222)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.948148\n",
      "Standard Deviation of Accuracies: 0.018543\n",
      "Iteration: 3, Model: XGBoost, SMOTE: False, Accuracy: 0.3296, Total Accuracy: 0.9481\n",
      "\n",
      "Overall Accuracy of Preferences: 0.975309\n",
      "Standard Deviation of Accuracies: 0.013703\n",
      "(0.9753086419753089, 0.013703147580752195)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.975309\n",
      "Standard Deviation of Accuracies: 0.013703\n",
      "Iteration: 3, Model: Random Forest, SMOTE: False, Accuracy: 0.3083, Total Accuracy: 0.9753\n",
      "\n",
      "Overall Accuracy of Preferences: 0.953086\n",
      "Standard Deviation of Accuracies: 0.019158\n",
      "(0.9530864197530863, 0.019157693571625493)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.953086\n",
      "Standard Deviation of Accuracies: 0.019158\n",
      "Iteration: 3, Model: Gradient Boosting, SMOTE: False, Accuracy: 0.3209, Total Accuracy: 0.9531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.887654\n",
      "Standard Deviation of Accuracies: 0.026782\n",
      "(0.8876543209876544, 0.026781872769435968)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.887654\n",
      "Standard Deviation of Accuracies: 0.026782\n",
      "Iteration: 3, Model: AdaBoost, SMOTE: False, Accuracy: 0.3787, Total Accuracy: 0.8877\n",
      "\n",
      "Overall Accuracy of Preferences: 0.951358\n",
      "Standard Deviation of Accuracies: 0.024017\n",
      "(0.951358024691358, 0.024016709115114238)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.951358\n",
      "Standard Deviation of Accuracies: 0.024017\n",
      "Iteration: 3, Model: K-Nearest Neighbors, SMOTE: False, Accuracy: 0.3194, Total Accuracy: 0.9514\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980741\n",
      "Standard Deviation of Accuracies: 0.010751\n",
      "(0.9807407407407408, 0.010751378299791236)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980741\n",
      "Standard Deviation of Accuracies: 0.010751\n",
      "Iteration: 3, Model: Decision Tree, SMOTE: False, Accuracy: 0.3075, Total Accuracy: 0.9807\n",
      "\n",
      "Overall Accuracy of Preferences: 0.847407\n",
      "Standard Deviation of Accuracies: 0.036819\n",
      "(0.8474074074074076, 0.03681911195330111)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.847407\n",
      "Standard Deviation of Accuracies: 0.036819\n",
      "Iteration: 3, Model: Gaussian Naive Bayes, SMOTE: False, Accuracy: 0.4119, Total Accuracy: 0.8474\n",
      "\n",
      "Overall Accuracy of Preferences: 0.882716\n",
      "Standard Deviation of Accuracies: 0.023586\n",
      "(0.8827160493827163, 0.023586386635238016)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.882716\n",
      "Standard Deviation of Accuracies: 0.023586\n",
      "Iteration: 3, Model: Stochastic Gradient Descent, SMOTE: False, Accuracy: 0.3755, Total Accuracy: 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.982716\n",
      "Standard Deviation of Accuracies: 0.010875\n",
      "(0.9827160493827163, 0.010875415084219879)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.982716\n",
      "Standard Deviation of Accuracies: 0.010875\n",
      "Iteration: 3, Model: MLP Classifier, SMOTE: False, Accuracy: 0.3051, Total Accuracy: 0.9827\n",
      "\n",
      "Overall Accuracy of Preferences: 0.887160\n",
      "Standard Deviation of Accuracies: 0.030463\n",
      "(0.8871604938271604, 0.030462572076494495)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.887160\n",
      "Standard Deviation of Accuracies: 0.030463\n",
      "Iteration: 4, Model: Logistic Regression, SMOTE: True, Accuracy: 0.3652, Total Accuracy: 0.8872\n",
      "\n",
      "Overall Accuracy of Preferences: 0.975802\n",
      "Standard Deviation of Accuracies: 0.014046\n",
      "(0.9758024691358026, 0.014045889040323847)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.975802\n",
      "Standard Deviation of Accuracies: 0.014046\n",
      "Iteration: 4, Model: Support Vector Machine, SMOTE: True, Accuracy: 0.3225, Total Accuracy: 0.9758\n",
      "\n",
      "Overall Accuracy of Preferences: 0.951852\n",
      "Standard Deviation of Accuracies: 0.019945\n",
      "(0.9518518518518518, 0.019945054841238888)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.951852\n",
      "Standard Deviation of Accuracies: 0.019945\n",
      "Iteration: 4, Model: XGBoost, SMOTE: True, Accuracy: 0.3510, Total Accuracy: 0.9519\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980988\n",
      "Standard Deviation of Accuracies: 0.012199\n",
      "(0.9809876543209877, 0.012199130199541375)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980988\n",
      "Standard Deviation of Accuracies: 0.012199\n",
      "Iteration: 4, Model: Random Forest, SMOTE: True, Accuracy: 0.3162, Total Accuracy: 0.9810\n",
      "\n",
      "Overall Accuracy of Preferences: 0.960000\n",
      "Standard Deviation of Accuracies: 0.018504\n",
      "(0.9600000000000001, 0.018503697773032273)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.960000\n",
      "Standard Deviation of Accuracies: 0.018504\n",
      "Iteration: 4, Model: Gradient Boosting, SMOTE: True, Accuracy: 0.3281, Total Accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.880000\n",
      "Standard Deviation of Accuracies: 0.022090\n",
      "(0.8800000000000001, 0.022090142465525955)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.880000\n",
      "Standard Deviation of Accuracies: 0.022090\n",
      "Iteration: 4, Model: AdaBoost, SMOTE: True, Accuracy: 0.3874, Total Accuracy: 0.8800\n",
      "\n",
      "Overall Accuracy of Preferences: 0.956790\n",
      "Standard Deviation of Accuracies: 0.021132\n",
      "(0.9567901234567905, 0.02113239847978232)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.956790\n",
      "Standard Deviation of Accuracies: 0.021132\n",
      "Iteration: 4, Model: K-Nearest Neighbors, SMOTE: True, Accuracy: 0.3241, Total Accuracy: 0.9568\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980988\n",
      "Standard Deviation of Accuracies: 0.014400\n",
      "(0.9809876543209876, 0.01439952919686255)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980988\n",
      "Standard Deviation of Accuracies: 0.014400\n",
      "Iteration: 4, Model: Decision Tree, SMOTE: True, Accuracy: 0.3099, Total Accuracy: 0.9810\n",
      "\n",
      "Overall Accuracy of Preferences: 0.858765\n",
      "Standard Deviation of Accuracies: 0.036285\n",
      "(0.8587654320987655, 0.03628537670631376)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.858765\n",
      "Standard Deviation of Accuracies: 0.036285\n",
      "Iteration: 4, Model: Gaussian Naive Bayes, SMOTE: True, Accuracy: 0.4213, Total Accuracy: 0.8588\n",
      "\n",
      "Overall Accuracy of Preferences: 0.892593\n",
      "Standard Deviation of Accuracies: 0.029366\n",
      "(0.8925925925925926, 0.029366112125728983)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.892593\n",
      "Standard Deviation of Accuracies: 0.029366\n",
      "Iteration: 4, Model: Stochastic Gradient Descent, SMOTE: True, Accuracy: 0.3557, Total Accuracy: 0.8926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.982963\n",
      "Standard Deviation of Accuracies: 0.011657\n",
      "(0.9829629629629629, 0.011657354700358396)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.982963\n",
      "Standard Deviation of Accuracies: 0.011657\n",
      "Iteration: 4, Model: MLP Classifier, SMOTE: True, Accuracy: 0.3091, Total Accuracy: 0.9830\n",
      "\n",
      "Overall Accuracy of Preferences: 0.890370\n",
      "Standard Deviation of Accuracies: 0.026391\n",
      "(0.8903703703703705, 0.026390892278296073)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.890370\n",
      "Standard Deviation of Accuracies: 0.026391\n",
      "Iteration: 4, Model: Logistic Regression, SMOTE: False, Accuracy: 0.3755, Total Accuracy: 0.8904\n",
      "\n",
      "Overall Accuracy of Preferences: 0.976790\n",
      "Standard Deviation of Accuracies: 0.011108\n",
      "(0.9767901234567901, 0.011108367288100732)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.976790\n",
      "Standard Deviation of Accuracies: 0.011108\n",
      "Iteration: 4, Model: Support Vector Machine, SMOTE: False, Accuracy: 0.3012, Total Accuracy: 0.9768\n",
      "\n",
      "Overall Accuracy of Preferences: 0.955802\n",
      "Standard Deviation of Accuracies: 0.020103\n",
      "(0.9558024691358026, 0.02010337557465171)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.955802\n",
      "Standard Deviation of Accuracies: 0.020103\n",
      "Iteration: 4, Model: XGBoost, SMOTE: False, Accuracy: 0.3209, Total Accuracy: 0.9558\n",
      "\n",
      "Overall Accuracy of Preferences: 0.982963\n",
      "Standard Deviation of Accuracies: 0.011177\n",
      "(0.982962962962963, 0.011176760787907448)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.982963\n",
      "Standard Deviation of Accuracies: 0.011177\n",
      "Iteration: 4, Model: Random Forest, SMOTE: False, Accuracy: 0.3130, Total Accuracy: 0.9830\n",
      "\n",
      "Overall Accuracy of Preferences: 0.960741\n",
      "Standard Deviation of Accuracies: 0.019044\n",
      "(0.9607407407407408, 0.019044385381011004)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.960741\n",
      "Standard Deviation of Accuracies: 0.019044\n",
      "Iteration: 4, Model: Gradient Boosting, SMOTE: False, Accuracy: 0.3312, Total Accuracy: 0.9607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.897284\n",
      "Standard Deviation of Accuracies: 0.022930\n",
      "(0.897283950617284, 0.022929751637024976)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.897284\n",
      "Standard Deviation of Accuracies: 0.022930\n",
      "Iteration: 4, Model: AdaBoost, SMOTE: False, Accuracy: 0.3597, Total Accuracy: 0.8973\n",
      "\n",
      "Overall Accuracy of Preferences: 0.957531\n",
      "Standard Deviation of Accuracies: 0.016992\n",
      "(0.9575308641975309, 0.016992247441071837)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.957531\n",
      "Standard Deviation of Accuracies: 0.016992\n",
      "Iteration: 4, Model: K-Nearest Neighbors, SMOTE: False, Accuracy: 0.3304, Total Accuracy: 0.9575\n",
      "\n",
      "Overall Accuracy of Preferences: 0.982222\n",
      "Standard Deviation of Accuracies: 0.013333\n",
      "(0.982222222222222, 0.013333333333333343)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.982222\n",
      "Standard Deviation of Accuracies: 0.013333\n",
      "Iteration: 4, Model: Decision Tree, SMOTE: False, Accuracy: 0.3138, Total Accuracy: 0.9822\n",
      "\n",
      "Overall Accuracy of Preferences: 0.857778\n",
      "Standard Deviation of Accuracies: 0.038127\n",
      "(0.8577777777777776, 0.038127166710039725)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.857778\n",
      "Standard Deviation of Accuracies: 0.038127\n",
      "Iteration: 4, Model: Gaussian Naive Bayes, SMOTE: False, Accuracy: 0.4158, Total Accuracy: 0.8578\n",
      "\n",
      "Overall Accuracy of Preferences: 0.901975\n",
      "Standard Deviation of Accuracies: 0.023321\n",
      "(0.9019753086419754, 0.023321245807641222)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.901975\n",
      "Standard Deviation of Accuracies: 0.023321\n",
      "Iteration: 4, Model: Stochastic Gradient Descent, SMOTE: False, Accuracy: 0.3462, Total Accuracy: 0.9020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.981235\n",
      "Standard Deviation of Accuracies: 0.012648\n",
      "(0.9812345679012348, 0.012648146641842665)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.981235\n",
      "Standard Deviation of Accuracies: 0.012648\n",
      "Iteration: 4, Model: MLP Classifier, SMOTE: False, Accuracy: 0.3130, Total Accuracy: 0.9812\n",
      "\n",
      "Overall Accuracy of Preferences: 0.886667\n",
      "Standard Deviation of Accuracies: 0.028313\n",
      "(0.8866666666666666, 0.028313355338690952)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.886667\n",
      "Standard Deviation of Accuracies: 0.028313\n",
      "Iteration: 5, Model: Logistic Regression, SMOTE: True, Accuracy: 0.3628, Total Accuracy: 0.8867\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980000\n",
      "Standard Deviation of Accuracies: 0.012269\n",
      "(0.9800000000000002, 0.012268897035856645)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980000\n",
      "Standard Deviation of Accuracies: 0.012269\n",
      "Iteration: 5, Model: Support Vector Machine, SMOTE: True, Accuracy: 0.3154, Total Accuracy: 0.9800\n",
      "\n",
      "Overall Accuracy of Preferences: 0.945185\n",
      "Standard Deviation of Accuracies: 0.027089\n",
      "(0.9451851851851851, 0.02708856918894316)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.945185\n",
      "Standard Deviation of Accuracies: 0.027089\n",
      "Iteration: 5, Model: XGBoost, SMOTE: True, Accuracy: 0.3937, Total Accuracy: 0.9452\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980247\n",
      "Standard Deviation of Accuracies: 0.011529\n",
      "(0.980246913580247, 0.011528511140670382)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980247\n",
      "Standard Deviation of Accuracies: 0.011529\n",
      "Iteration: 5, Model: Random Forest, SMOTE: True, Accuracy: 0.3233, Total Accuracy: 0.9802\n",
      "\n",
      "Overall Accuracy of Preferences: 0.960741\n",
      "Standard Deviation of Accuracies: 0.025094\n",
      "(0.9607407407407407, 0.025094243943657574)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.960741\n",
      "Standard Deviation of Accuracies: 0.025094\n",
      "Iteration: 5, Model: Gradient Boosting, SMOTE: True, Accuracy: 0.3526, Total Accuracy: 0.9607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.877778\n",
      "Standard Deviation of Accuracies: 0.026895\n",
      "(0.8777777777777778, 0.026895452043966303)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.877778\n",
      "Standard Deviation of Accuracies: 0.026895\n",
      "Iteration: 5, Model: AdaBoost, SMOTE: True, Accuracy: 0.3621, Total Accuracy: 0.8778\n",
      "\n",
      "Overall Accuracy of Preferences: 0.959506\n",
      "Standard Deviation of Accuracies: 0.025064\n",
      "(0.9595061728395062, 0.025063856870023736)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.959506\n",
      "Standard Deviation of Accuracies: 0.025064\n",
      "Iteration: 5, Model: K-Nearest Neighbors, SMOTE: True, Accuracy: 0.3439, Total Accuracy: 0.9595\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980000\n",
      "Standard Deviation of Accuracies: 0.013944\n",
      "(0.9799999999999999, 0.013943513978760985)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980000\n",
      "Standard Deviation of Accuracies: 0.013944\n",
      "Iteration: 5, Model: Decision Tree, SMOTE: True, Accuracy: 0.3170, Total Accuracy: 0.9800\n",
      "\n",
      "Overall Accuracy of Preferences: 0.847654\n",
      "Standard Deviation of Accuracies: 0.031057\n",
      "(0.8476543209876544, 0.031057174488060436)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.847654\n",
      "Standard Deviation of Accuracies: 0.031057\n",
      "Iteration: 5, Model: Gaussian Naive Bayes, SMOTE: True, Accuracy: 0.4324, Total Accuracy: 0.8477\n",
      "\n",
      "Overall Accuracy of Preferences: 0.872346\n",
      "Standard Deviation of Accuracies: 0.022685\n",
      "(0.8723456790123457, 0.022685164189122718)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.872346\n",
      "Standard Deviation of Accuracies: 0.022685\n",
      "Iteration: 5, Model: Stochastic Gradient Descent, SMOTE: True, Accuracy: 0.4016, Total Accuracy: 0.8723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.984938\n",
      "Standard Deviation of Accuracies: 0.009655\n",
      "(0.9849382716049383, 0.009654920886922052)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.984938\n",
      "Standard Deviation of Accuracies: 0.009655\n",
      "Iteration: 5, Model: MLP Classifier, SMOTE: True, Accuracy: 0.3154, Total Accuracy: 0.9849\n",
      "\n",
      "Overall Accuracy of Preferences: 0.883210\n",
      "Standard Deviation of Accuracies: 0.024395\n",
      "(0.88320987654321, 0.024394511914631874)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.883210\n",
      "Standard Deviation of Accuracies: 0.024395\n",
      "Iteration: 5, Model: Logistic Regression, SMOTE: False, Accuracy: 0.3628, Total Accuracy: 0.8832\n",
      "\n",
      "Overall Accuracy of Preferences: 0.976543\n",
      "Standard Deviation of Accuracies: 0.017745\n",
      "(0.9765432098765433, 0.01774516899637625)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.976543\n",
      "Standard Deviation of Accuracies: 0.017745\n",
      "Iteration: 5, Model: Support Vector Machine, SMOTE: False, Accuracy: 0.3162, Total Accuracy: 0.9765\n",
      "\n",
      "Overall Accuracy of Preferences: 0.954321\n",
      "Standard Deviation of Accuracies: 0.023664\n",
      "(0.9543209876543212, 0.023663803867374225)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.954321\n",
      "Standard Deviation of Accuracies: 0.023664\n",
      "Iteration: 5, Model: XGBoost, SMOTE: False, Accuracy: 0.3542, Total Accuracy: 0.9543\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980247\n",
      "Standard Deviation of Accuracies: 0.013836\n",
      "(0.980246913580247, 0.0138359760257994)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.980247\n",
      "Standard Deviation of Accuracies: 0.013836\n",
      "Iteration: 5, Model: Random Forest, SMOTE: False, Accuracy: 0.3202, Total Accuracy: 0.9802\n",
      "\n",
      "Overall Accuracy of Preferences: 0.958025\n",
      "Standard Deviation of Accuracies: 0.024142\n",
      "(0.9580246913580247, 0.024142037652504316)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.958025\n",
      "Standard Deviation of Accuracies: 0.024142\n",
      "Iteration: 5, Model: Gradient Boosting, SMOTE: False, Accuracy: 0.3423, Total Accuracy: 0.9580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.881481\n",
      "Standard Deviation of Accuracies: 0.028943\n",
      "(0.8814814814814816, 0.028942653357219957)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.881481\n",
      "Standard Deviation of Accuracies: 0.028943\n",
      "Iteration: 5, Model: AdaBoost, SMOTE: False, Accuracy: 0.3399, Total Accuracy: 0.8815\n",
      "\n",
      "Overall Accuracy of Preferences: 0.951358\n",
      "Standard Deviation of Accuracies: 0.022114\n",
      "(0.951358024691358, 0.022113589069192478)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.951358\n",
      "Standard Deviation of Accuracies: 0.022114\n",
      "Iteration: 5, Model: K-Nearest Neighbors, SMOTE: False, Accuracy: 0.3281, Total Accuracy: 0.9514\n",
      "\n",
      "Overall Accuracy of Preferences: 0.981975\n",
      "Standard Deviation of Accuracies: 0.010766\n",
      "(0.9819753086419751, 0.010765545361279829)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.981975\n",
      "Standard Deviation of Accuracies: 0.010766\n",
      "Iteration: 5, Model: Decision Tree, SMOTE: False, Accuracy: 0.3202, Total Accuracy: 0.9820\n",
      "\n",
      "Overall Accuracy of Preferences: 0.846420\n",
      "Standard Deviation of Accuracies: 0.029190\n",
      "(0.8464197530864197, 0.02919015645655635)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.846420\n",
      "Standard Deviation of Accuracies: 0.029190\n",
      "Iteration: 5, Model: Gaussian Naive Bayes, SMOTE: False, Accuracy: 0.4071, Total Accuracy: 0.8464\n",
      "\n",
      "Overall Accuracy of Preferences: 0.884444\n",
      "Standard Deviation of Accuracies: 0.029790\n",
      "(0.8844444444444446, 0.029789691124653177)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.884444\n",
      "Standard Deviation of Accuracies: 0.029790\n",
      "Iteration: 5, Model: Stochastic Gradient Descent, SMOTE: False, Accuracy: 0.3597, Total Accuracy: 0.8844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurie2905/anaconda3/envs/masterEnv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy of Preferences: 0.985185\n",
      "Standard Deviation of Accuracies: 0.009370\n",
      "(0.9851851851851852, 0.009369711585684089)\n",
      "\n",
      "Overall Accuracy of Preferences: 0.985185\n",
      "Standard Deviation of Accuracies: 0.009370\n",
      "Iteration: 5, Model: MLP Classifier, SMOTE: False, Accuracy: 0.3170, Total Accuracy: 0.9852\n",
      "     Iteration  SMOTE                   Model        Metric     Score   Rank\n",
      "35           0   True    Gaussian Naive Bayes      accuracy  0.433202    1.0\n",
      "387          4   True    Gaussian Naive Bayes      accuracy  0.432411    2.0\n",
      "79           0  False    Gaussian Naive Bayes      accuracy  0.426877    3.0\n",
      "299          3   True    Gaussian Naive Bayes      accuracy  0.421344    4.0\n",
      "211          2   True    Gaussian Naive Bayes      accuracy  0.418182    5.0\n",
      "..         ...    ...                     ...           ...       ...    ...\n",
      "5            0   True  Support Vector Machine  recall_macro  0.324905  106.0\n",
      "93           1   True  Support Vector Machine  recall_macro  0.322290  107.0\n",
      "129          1   True          MLP Classifier  recall_macro  0.321162  108.0\n",
      "165          1  False    Gaussian Naive Bayes  recall_macro  0.317389  109.0\n",
      "121          1   True    Gaussian Naive Bayes  recall_macro  0.305344  110.0\n",
      "\n",
      "[440 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from models.preferences.prediction import PreferenceModel\n",
    "import os\n",
    "from utils.process_data import get_data\n",
    "from models.preferences.preference_utils import print_preference_difference_and_accuracy\n",
    "from models.preferences.preference_utils import get_child_data, initialize_child_preference_data\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver='liblinear', C=1.0, max_iter=10000, class_weight='balanced'),\n",
    "    \"Support Vector Machine\": SVC(C=1.0, kernel='rbf', probability=True, class_weight='balanced'),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, eval_metric='logloss', scale_pos_weight=1),  # XGBoost uses scale_pos_weight for class balancing\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=None, criterion='gini', class_weight='balanced'),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),  # No direct class_weight support\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50, learning_rate=1.0, algorithm='SAMME.R'),  # No direct class_weight support\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto'),  # No direct class_weight support\n",
    "    \"Decision Tree\": DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, class_weight='balanced'),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),  # No direct class_weight support\n",
    "    \"Stochastic Gradient Descent\": SGDClassifier(loss='hinge', alpha=0.0001, max_iter=2000, tol=1e-3, class_weight='balanced'),\n",
    "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=2000)  # No direct class_weight support\n",
    "}\n",
    "\n",
    "\n",
    "# Define scorers with `average='macro'` set only for the appropriate metrics\n",
    "scorers = {\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division=0),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro', zero_division=0),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro', zero_division=0),\n",
    "    'accuracy': make_scorer(accuracy_score)  # No `average` parameter here\n",
    "}\n",
    "\n",
    "child_feature_data = get_child_data()\n",
    "ingredient_df = get_data(\"data.csv\")\n",
    "\n",
    "# Initial prediction of preferences\n",
    "file_path = os.path.join('', \"preferences_visualization.png\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, make_scorer\n",
    "\n",
    "# Assuming all imports and other necessary function definitions have been done as in the provided code.\n",
    "\n",
    "def evaluate_models():\n",
    "    # Evaluate models using cross-validation\n",
    "    results = []\n",
    "    \n",
    "    for iter in range(5):\n",
    "        initial_preference = initialize_child_preference_data(child_feature_data, ingredient_df, split = 0.7, seed=None, plot_graphs=False)\n",
    "        for apply_SMOTE in [True, False]:\n",
    "            for name, model in models.items():\n",
    "                predictor = PreferenceModel(\n",
    "                    ingredient_df, child_feature_data, initial_preference, model_name=name, visualize_data=False, apply_SMOTE=apply_SMOTE, file_path=file_path, seed=None\n",
    "                )\n",
    "                updated_preferences, true_labels, predicted_labels = predictor.run_pipeline()\n",
    "                print(print_preference_difference_and_accuracy(initial_preference, updated_preferences, summary_only=True))\n",
    "                accuracy_total, std = print_preference_difference_and_accuracy(initial_preference, updated_preferences, summary_only=True)\n",
    "                # Calculate accuracy\n",
    "                accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "                # Print the model, accuracy, and iteration\n",
    "                print(f\"Iteration: {iter + 1}, Model: {name}, SMOTE: {apply_SMOTE}, Accuracy: {accuracy:.4f}, Total Accuracy: {accuracy_total:.4f}\")\n",
    "\n",
    "                # Loop over each scorer\n",
    "                for scorer_name, scorer in scorers.items():\n",
    "                    # Calculate the score using the scorer\n",
    "                    if scorer_name in ['precision_macro', 'recall_macro', 'f1_macro']:\n",
    "                        score = scorer._score_func(true_labels, predicted_labels, average='macro')\n",
    "                    else:\n",
    "                        score = scorer._score_func(true_labels, predicted_labels)\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"Iteration\": iter,\n",
    "                        \"SMOTE\": apply_SMOTE,\n",
    "                        \"Model\": name,\n",
    "                        \"Metric\": scorer_name,\n",
    "                        \"Score\": score\n",
    "                    })\n",
    "\n",
    "    # Convert results to DataFrame for easy comparison\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def get_ranked_models(results_df):\n",
    "    # Rank models for each metric\n",
    "    ranked_results = results_df.copy()\n",
    "    ranked_results['Rank'] = ranked_results.groupby('Metric')['Score'].rank(ascending=False, method='min')\n",
    "\n",
    "    # Convert results to DataFrame for easy comparison\n",
    "    results_df_sorted = ranked_results.sort_values(by=['Metric', 'Rank'])\n",
    "\n",
    "    print(results_df_sorted)\n",
    "    return results_df_sorted\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "results_df = evaluate_models()\n",
    "sorted_results_df = get_ranked_models(results_df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `sorted_results_df` is your sorted DataFrame from the previous code\n",
    "output_file_path = \"sorted_results.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "sorted_results_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Sorted results have been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "import os\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from models.preferences.prediction import PreferenceModel\n",
    "from models.preferences.preference_utils import print_preference_difference_and_accuracy\n",
    "from models.preferences.preference_utils import get_child_data, initialize_child_preference_data\n",
    "from utils.process_data import get_data\n",
    "\n",
    "# Define models with initial hyperparameters\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver='liblinear', C=1.0, max_iter=10000, class_weight='balanced'),\n",
    "    \"Support Vector Machine\": SVC(C=1.0, kernel='rbf', probability=True, class_weight='balanced'),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, eval_metric='logloss', scale_pos_weight=1),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=None, criterion='gini', class_weight='balanced'),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "}\n",
    "\n",
    "# Define scorers\n",
    "scorers = {\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division=0),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro', zero_division=0),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro', zero_division=0),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "}\n",
    "\n",
    "# Load and preprocess data\n",
    "child_feature_data = get_child_data()\n",
    "ingredient_df = get_data(\"data.csv\")\n",
    "file_path = os.path.join('', \"preferences_visualization.png\")\n",
    "\n",
    "# Function to optimize hyperparameters using Optuna\n",
    "def optimize_hyperparameters(model_name, ingredient_df, child_feature_data, n_trials=50):\n",
    "    def objective(trial):\n",
    "        # Re-initialize preferences for each trial\n",
    "        initial_preference = initialize_child_preference_data(child_feature_data, ingredient_df, split=0.5, seed=None, plot_graphs=False)\n",
    "        \n",
    "        # Define the hyperparameter search space\n",
    "        if model_name == \"XGBoost\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "                'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 1.0, 3.0),\n",
    "                'eval_metric': 'mlogloss',\n",
    "                'use_label_encoder': False\n",
    "            }\n",
    "            model = XGBClassifier(**params)\n",
    "        elif model_name == \"Random Forest\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "                'class_weight': 'balanced'\n",
    "            }\n",
    "            model = RandomForestClassifier(**params)\n",
    "        elif model_name == \"Support Vector Machine\":\n",
    "            params = {\n",
    "                'C': trial.suggest_loguniform('C', 1e-3, 1e2),\n",
    "                'kernel': trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly']),\n",
    "                'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e-1),\n",
    "                'probability': True,\n",
    "                'class_weight': 'balanced'\n",
    "            }\n",
    "            model = SVC(**params)\n",
    "        elif model_name == \"Logistic Regression\":\n",
    "            params = {\n",
    "                'C': trial.suggest_loguniform('C', 1e-3, 1e2),\n",
    "                'solver': trial.suggest_categorical('solver', ['liblinear', 'lbfgs', 'sag']),\n",
    "                'penalty': 'l2',\n",
    "                'class_weight': 'balanced',\n",
    "                'max_iter': 10000\n",
    "            }\n",
    "            model = LogisticRegression(**params)\n",
    "        elif model_name == \"Gradient Boosting\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])\n",
    "            }\n",
    "            model = GradientBoostingClassifier(**params)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        file_path = ''\n",
    "        # Re-run the preference model with the new hyperparameters\n",
    "        predictor = PreferenceModel(\n",
    "            ingredient_df, child_feature_data, initial_preference, model_name = model_name, visualize_data=False, apply_SMOTE=True, file_path=file_path, seed=None\n",
    "        )\n",
    "        updated_known_and_predicted_preferences = predictor.run_pipeline()\n",
    "        \n",
    "        accuracy, _, _, _ = print_preference_difference_and_accuracy(\n",
    "            initial_preference, updated_known_and_predicted_preferences, summary_only=True\n",
    "        )\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    \n",
    "    print(f\"\\nBest hyperparameters for {model_name}:\")\n",
    "    print(study.best_params)\n",
    "    print(f\"Best cross-validation accuracy: {study.best_value:.4f}\")\n",
    "    \n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "# Main execution\n",
    "top_3_models = sorted_results_df['Model'].value_counts().index[:3].tolist()\n",
    "\n",
    "# Initialize preferences for Optuna optimization\n",
    "initial_preference = initialize_child_preference_data(child_feature_data, ingredient_df, split=0.5, seed=None, plot_graphs=False)\n",
    "\n",
    "# Optimize hyperparameters for each of the top 3 models\n",
    "optimized_models = {}\n",
    "for model_name in top_3_models:\n",
    "    print(f\"\\nOptimizing hyperparameters for {model_name}...\")\n",
    "    best_params, best_score = optimize_hyperparameters(model_name, ingredient_df, child_feature_data, initial_preference)\n",
    "    optimized_models[model_name] = {\n",
    "        'best_params': best_params,\n",
    "        'best_score': best_score\n",
    "    }\n",
    "\n",
    "# Display optimized hyperparameters and scores\n",
    "print(\"\\nOptimized Hyperparameters and Scores for Top 3 Models:\")\n",
    "for model_name in optimized_models:\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Best Hyperparameters: {optimized_models[model_name]['best_params']}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {optimized_models[model_name]['best_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
