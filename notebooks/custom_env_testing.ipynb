{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "from models.envs.env import BaseEnvironment\n",
    "\n",
    "class SchoolMealSelectionDiscrete(BaseEnvironment):\n",
    "    \"\"\"\n",
    "    A custom Gym environment for selecting school meals that meet certain nutritional and environmental criteria.\n",
    "    The environment allows actions to adjust the quantity of ingredients and calculates rewards based on multiple metrics,\n",
    "    such as nutrient values, environmental impact, cost, and consumption patterns.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], 'render_fps': 1}\n",
    "\n",
    "    def __init__(self, ingredient_df, max_ingredients: int = 6, action_scaling_factor: int = 10, render_mode: str = None, \n",
    "                 verbose: int = 0, seed: int = None, reward_calculator_type: str = 'sparse', \n",
    "                 initialization_strategy: str = 'zero', max_episode_steps: int = 100):\n",
    "        super().__init__(ingredient_df, max_ingredients, action_scaling_factor, render_mode, verbose, seed, reward_calculator_type, initialization_strategy, max_episode_steps)\n",
    "        \n",
    "        # Define the action space: 0 (set to zero), 1 (stay the same), 2 (decrease), 3 (increase)\n",
    "        self.initialize_action_space()\n",
    "        \n",
    "        self.invalid_actions: List[int] = []\n",
    "        self.n_invalid_actions = 0  # You can set this dynamically as needed\n",
    "        self.possible_actions = np.arange(4 * self.n_ingredients)\n",
    "\n",
    "    def initialize_action_space(self) -> None:\n",
    "        self.action_space = gym.spaces.MultiDiscrete([4] * self.n_ingredients)\n",
    "        \n",
    "    def update_selection(self, action: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Update the current selection of ingredients based on the action.\n",
    "        \"\"\"\n",
    "        for i in range(len(action)):\n",
    "            if action[i] == 0:\n",
    "                self.current_selection[i] = 0\n",
    "            elif action[i] == 1:\n",
    "                continue  # No change\n",
    "            elif action[i] == 2:\n",
    "                self.current_selection[i] = max(0, self.current_selection[i] - self.action_scaling_factor)\n",
    "            elif action[i] == 3:\n",
    "                self.current_selection[i] += self.action_scaling_factor\n",
    "\n",
    "        # Ensure that the number of selected ingredients does not exceed the maximum allowed\n",
    "        num_selected_ingredients = np.sum(self.current_selection > 0)\n",
    "        if num_selected_ingredients > self.max_ingredients:\n",
    "            excess_indices = np.argsort(-self.current_selection)\n",
    "            self.current_selection[excess_indices[self.max_ingredients:]] = 0\n",
    "            print(\"current_selection was cut\")\n",
    "\n",
    "    def validate_action_shape(self, action: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Validate the shape of the action array.\n",
    "        \"\"\"\n",
    "        if self.verbose > 1:\n",
    "            if not isinstance(action, np.ndarray) or action.shape != (self.n_ingredients,):\n",
    "                raise ValueError(f\"Expected action to be an np.ndarray of shape {(self.n_ingredients,)}, but got {type(action)} with shape {action.shape}\")\n",
    "            if action.shape != self.action_space.shape:\n",
    "                raise ValueError(f\"Action shape {action.shape} is not equal to action space shape {self.action_space.shape}\")\n",
    "\n",
    "    def _choose_invalid_actions(self) -> None:\n",
    "        \"\"\"\n",
    "        Randomly choose invalid actions that are not the current state.\n",
    "        \"\"\"\n",
    "        self.state = self.action_space.sample()\n",
    "        converted_state: List[int] = []\n",
    "        running_total = 0\n",
    "        for i in range(len(self.action_space.nvec)):\n",
    "            converted_state.append(running_total + self.state[i])\n",
    "            running_total += self.action_space.nvec[i]\n",
    "\n",
    "        # Randomly choose invalid actions that are not the current state\n",
    "        potential_invalid_actions = [i for i in self.possible_actions if i not in converted_state]\n",
    "        self.invalid_actions = np.random.choice(potential_invalid_actions, self.n_invalid_actions, replace=False).tolist()\n",
    "\n",
    "    def action_masks(self) -> List[bool]:\n",
    "        \"\"\"\n",
    "        Generate action masks indicating valid actions.\n",
    "        \"\"\"\n",
    "        self._choose_invalid_actions()\n",
    "        return [action not in self.invalid_actions for action in self.possible_actions]\n",
    "\n",
    "# Example usage\n",
    "from utils.process_data import get_data\n",
    "ingredient_df = get_data()  # Replace with your actual dataframe\n",
    "n_ingredients = 26\n",
    "max_ingredients = 5\n",
    "env = SchoolMealSelectionDiscrete(ingredient_df, max_ingredients)\n",
    "\n",
    "# Reset the environment\n",
    "observation = env.reset()\n",
    "print(\"Initial observation:\", observation)\n",
    "\n",
    "# Get the action mask\n",
    "action_mask = env.action_masks()\n",
    "print(\"Action mask:\", action_mask)\n",
    "print(\"Length of Action mask:\", len(action_mask))\n",
    "# Take a step with a sample action\n",
    "action = env.action_space.sample()\n",
    "print(\"Action:\", action)\n",
    "\n",
    "observation, reward, done, truncated, info = env.step(action)\n",
    "print(\"Observation after step:\", observation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(10 + 4, dtype=np.int8)\n",
    "x[-4:] = -1\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "n_ingredients = 2\n",
    "\n",
    "multi_discrete_space1 = gym.spaces.MultiDiscrete([4, 2, n_ingredients])\n",
    "\n",
    "print(multi_discrete_space1)\n",
    "\n",
    "# Sample from the MultiDiscrete space multiple times\n",
    "for _ in range(5):\n",
    "    sample = multi_discrete_space1.sample()\n",
    "    print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.process_data import get_data\n",
    "import random\n",
    "\n",
    "# Load data\n",
    "ingredient_df = get_data()\n",
    "n_ingredients = len(ingredient_df)\n",
    "\n",
    "# Extract group columns\n",
    "group_a_veg = ingredient_df['Group A veg'].values.astype(np.float32)\n",
    "group_a_fruit = ingredient_df['Group A fruit'].values.astype(np.float32)\n",
    "group_bc = ingredient_df['Group BC'].values.astype(np.float32)\n",
    "group_d = ingredient_df['Group D'].values.astype(np.float32)\n",
    "group_e = ingredient_df['Group E'].values.astype(np.float32)\n",
    "bread = ingredient_df['Bread'].values.astype(np.float32)\n",
    "confectionary = ingredient_df['Confectionary'].values.astype(np.float32)\n",
    "\n",
    "# Group info dictionary\n",
    "group_info = {\n",
    "    'fruit': {'indexes': np.nonzero(group_a_fruit)[0], 'probability': 0.8},\n",
    "    'veg': {'indexes': np.nonzero(group_a_veg)[0], 'probability': 0.8},\n",
    "    'protein': {'indexes': np.nonzero(group_bc)[0], 'probability': 0.8},\n",
    "    'carbs': {'indexes': np.nonzero(group_d)[0], 'probability': 0.8},\n",
    "    'dairy': {'indexes': np.nonzero(group_e)[0], 'probability': 0.8},\n",
    "    'bread': {'indexes': np.nonzero(bread)[0], 'probability': 0.8},\n",
    "    'confectionary': {'indexes': np.nonzero(confectionary)[0], 'probability': 0.01}\n",
    "}\n",
    "\n",
    "# Ingredient group count targets\n",
    "ingredient_group_count_targets = {\n",
    "    'fruit': 1,\n",
    "    'veg': 1,\n",
    "    'protein': 1,\n",
    "    'carbs': 1,\n",
    "    'dairy': 1,\n",
    "    'bread': 1,\n",
    "    'confectionary': 0\n",
    "}\n",
    "\n",
    "# Initialize variables\n",
    "current_selection = np.zeros(n_ingredients, dtype=np.float32)\n",
    "selected_indices = []\n",
    "\n",
    "# Select indices based on targets\n",
    "for key, value in ingredient_group_count_targets.items():\n",
    "    if value > 0:\n",
    "        selected_indices.extend(random.sample(list(group_info[key]['indexes']), value))\n",
    "\n",
    "selected_dict = {key: [] for key in ingredient_group_count_targets}\n",
    "\n",
    "for idx in selected_indices:\n",
    "    for category, info in group_info.items():\n",
    "        if idx in info['indexes']:\n",
    "            selected_dict[category].append(idx)\n",
    "            \n",
    "current_selection[selected_indices] = 1\n",
    "\n",
    "# Create action mask\n",
    "action_mask = np.zeros(n_ingredients * 2, dtype=np.int8)\n",
    "non_zero_mask = current_selection != 0\n",
    "\n",
    "# Calculate ingredient group count\n",
    "ingredient_group_count = {\n",
    "    'fruit': np.sum(group_a_fruit * non_zero_mask),\n",
    "    'veg': np.sum(group_a_veg * non_zero_mask),\n",
    "    'protein': np.sum(group_bc * non_zero_mask),\n",
    "    'carbs': np.sum(group_d * non_zero_mask),\n",
    "    'dairy': np.sum(group_e * non_zero_mask),\n",
    "    'bread': np.sum(bread * non_zero_mask),\n",
    "    'confectionary': np.sum(confectionary * non_zero_mask)\n",
    "}\n",
    "\n",
    "# Update action mask for selected indices only\n",
    "for key, value in ingredient_group_count.items():\n",
    "    target = ingredient_group_count_targets[key]\n",
    "    indexes = group_info[key]['indexes']\n",
    "    selected = [idx for idx in selected_indices if idx in indexes]\n",
    "    \n",
    "    if target == 0:\n",
    "        for idx in indexes:\n",
    "            action_mask[idx * 2: idx * 2 + 2] = [0, 0]\n",
    "        continue\n",
    "        \n",
    "    if value == target:\n",
    "        for idx in selected:\n",
    "            action_mask[idx * 2: idx * 2 + 2] = [1, 1]\n",
    "        for idx in indexes:\n",
    "            if idx not in selected:\n",
    "                action_mask[idx * 2: idx * 2 + 2] = [0, 0]\n",
    "    elif value < target:\n",
    "        for idx in selected:\n",
    "            action_mask[idx * 2: idx * 2 + 2] = [1, 1]\n",
    "        for idx in indexes:\n",
    "            if idx not in selected:\n",
    "                action_mask[idx * 2: idx * 2 + 2] = [1, 1]\n",
    "    else:\n",
    "        for idx in selected:\n",
    "            action_mask[idx * 2: idx * 2 + 2] = [1, 0]\n",
    "        for idx in indexes:\n",
    "            if idx not in selected:\n",
    "                action_mask[idx * 2: idx * 2 + 2] = [0, 0]\n",
    "\n",
    "action_mask = np.reshape(action_mask, (n_ingredients, 2))\n",
    "\n",
    "# Output results\n",
    "print(action_mask)\n",
    "\n",
    "mask = {}\n",
    "i = 0\n",
    "for l in action_mask:\n",
    "    mask[i] = l\n",
    "    i += 1\n",
    "  \n",
    "print(mask)  \n",
    "print(current_selection)\n",
    "print(ingredient_group_count)\n",
    "print(selected_dict)\n",
    "print(group_info['confectionary']['indexes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box, MultiDiscrete, Dict\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import os\n",
    "from utils.process_data import get_data\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "from gymnasium.wrappers import TimeLimit, NormalizeObservation, NormalizeReward\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from utils.train_utils import setup_environment, get_unique_directory, monitor_memory_usage, plot_reward_distribution, set_seed\n",
    "reward_dir, reward_prefix = get_unique_directory(\"saved_models/reward\", 'reward_test34', '')\n",
    "\n",
    "class Args:\n",
    "    reward_metrics = None\n",
    "    render_mode = None\n",
    "    num_envs = 1\n",
    "    plot_reward_history = False\n",
    "    max_episode_steps = 1000\n",
    "    verbose = 2\n",
    "    action_scaling_factor = 10\n",
    "    memory_monitor = True\n",
    "    gamma = 0.99\n",
    "    max_ingredients = 6\n",
    "    action_scaling_factor = 10\n",
    "    reward_save_interval = 1000\n",
    "    vecnorm_norm_obs = True\n",
    "    vecnorm_norm_reward = True\n",
    "    vecnorm_clip_obs = 10\n",
    "    vecnorm_clip_reward = 10\n",
    "    vecnorm_epsilon = 1e-8 \n",
    "    vecnorm_norm_obs_keys = None\n",
    "    ingredient_df = get_data(\"small_data.csv\")\n",
    "    seed = 10\n",
    "    env_name = 'SchoolMealSelection-v0'\n",
    "    perfect_initialize = 'perfect'\n",
    "    vecnorm_norm_obs_keys = ['current_selection_value', 'time_left', 'cost', 'consumption', 'co2g']\n",
    "args = Args()\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "reward_save_path = os.path.abspath(os.path.join(reward_dir, reward_prefix))\n",
    "\n",
    "env = setup_environment(args, reward_save_path=reward_save_path, eval=False)\n",
    "    \n",
    "# Assuming your state and observation space are set up as above\n",
    "state = OrderedDict([\n",
    "    ('co2g', np.array([0.00999999], dtype=np.float32)),\n",
    "    ('consumption', np.array([[0.00999125, 0.00984591]], dtype=np.float32)),\n",
    "    ('cost', np.array([[0.00884706]], dtype=np.float32)),\n",
    "    ('current_selection_index', np.array([[0, 2, 7, 13, 15, 25]])),\n",
    "    ('current_selection_value', np.array([[0.00999906, 0.00999981, 0.00999934, 0.00999984, 0.0099996, 0.00999808]], dtype=np.float32)),\n",
    "    ('environment_counts', np.array([[2, 3, 1, 2]])),\n",
    "    ('groups', np.array([[1, 1, 1, 1, 1, 1, 0]])),\n",
    "    ('nutrients', np.array([[690.27, 21.514, 7.534, 85.008995, 24.738998, 10.741, 35.691, 1.2396]], dtype=np.float32)),\n",
    "    ('time_left', np.array([[0.01]], dtype=np.float32))\n",
    "])\n",
    "\n",
    "for key, value in state.items():\n",
    "    space_contains = env.observation_space.spaces[key].contains(value)\n",
    "    print(f\"{key}: {space_contains}\")  # This will tell you which fields are not matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from utils.process_data import get_data\n",
    "from models.envs.env_working import SchoolMealSelection\n",
    "ingredient_df = get_data()\n",
    "\n",
    "gym.make(\"SchoolMealSelection-v0\", ingredient_df = ingredient_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environment\n",
    "from gymnasium.envs.registration import register\n",
    "import gymnasium.envs.registration\n",
    "\n",
    "env_dict = gymnasium.envs.registration.registry.env_specs.copy()\n",
    "\n",
    "print(env_dict)\n",
    "register(\n",
    "    id='SchoolMealSelection-v0',\n",
    "    entry_point='models.envs.env_working:SchoolMealSelection',\n",
    "    max_episode_steps=1000,  # Allow multiple steps per episode, adjust as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "from utils.train_utils import *\n",
    "\n",
    "def monitor_memory(stop_event, interval=1):\n",
    "    \"\"\"\n",
    "    Function to monitor and print memory usage at regular intervals.\n",
    "    \"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"Memory usage: {mem.percent}% used ({mem.used / (1024 ** 3):.2f} GB / {mem.total / (1024 ** 3):.2f} GB)\")\n",
    "        time.sleep(interval)\n",
    "\n",
    "# Create an event to stop the thread\n",
    "stop_event = threading.Event()\n",
    "\n",
    "# Start memory monitoring in a separate thread\n",
    "memory_thread = threading.Thread(target=monitor_memory, args=(stop_event,), daemon=True)\n",
    "memory_thread.start()\n",
    "\n",
    "# Load the reward distribution plot\n",
    "load_path = \"/home/laurie2905/Masters Thesis/Masters_RL/saved_models/reward/reward_test34_(7)\"\n",
    "plot_reward_distribution(load_path, save_plot_path=None, chunk_size=5000)\n",
    "\n",
    "# Signal the memory monitoring thread to stop and wait for it to finish\n",
    "stop_event.set()\n",
    "memory_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "from utils.train_utils import *\n",
    "\n",
    "def monitor_memory(stop_event, interval=1):\n",
    "    \"\"\"\n",
    "    Function to monitor and print memory usage at regular intervals.\n",
    "    \"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"Memory usage: {mem.percent}% used ({mem.used / (1024 ** 3):.2f} GB / {mem.total / (1024 ** 3):.2f} GB)\")\n",
    "        time.sleep(interval)\n",
    "\n",
    "# Create an event to stop the thread\n",
    "stop_event = threading.Event()\n",
    "\n",
    "# Start memory monitoring in a separate thread\n",
    "memory_thread = threading.Thread(target=monitor_memory, args=(stop_event,), daemon=True)\n",
    "memory_thread.start()\n",
    "\n",
    "# Load the reward distribution plot\n",
    "load_path = \"/home/laurie2905/Masters Thesis/Masters_RL/saved_models/reward/reward_test34_(7)\"\n",
    "plot_reward_distribution(load_path, save_plot_path=None, chunk_size=5000)\n",
    "\n",
    "# Signal the memory monitoring thread to stop and wait for it to finish\n",
    "stop_event.set()\n",
    "memory_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.process_data import get_data\n",
    "\n",
    "# Load data\n",
    "ingredient_df = get_data()\n",
    "\n",
    "# Define the mapping dictionary\n",
    "rating_to_int = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5}\n",
    "\n",
    "# Apply the mapping to the 'Animal Welfare Rating' column\n",
    "x = [rating_to_int[val] for val in ingredient_df['Animal Welfare Rating'].values]\n",
    "\n",
    "print(x)\n",
    "print(x[0])\n",
    "print(type(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Given values\n",
    "mean = 7.59490565\n",
    "standard_deviation = 11.22572649\n",
    "threshold = 0  # Define the threshold below which people won't eat\n",
    "value_in_meal_plan = 400\n",
    "num_people = 1000  # Example number of people\n",
    "\n",
    "# Generate random values from a normal distribution for each person\n",
    "consumption_values = np.random.normal(loc=mean, scale=standard_deviation, size=num_people)\n",
    "\n",
    "# Replace negative values with zero for realistic consumption values\n",
    "consumption_values = np.where(consumption_values < 0, 0, consumption_values)\n",
    "\n",
    "# Calculate the number of people not eating the ingredient\n",
    "num_not_eating = np.sum(consumption_values == 0)\n",
    "\n",
    "# Calculate the total expected consumption\n",
    "total_expected_consumption = np.sum(consumption_values)\n",
    "\n",
    "# Calculate the expected food waste\n",
    "total_food_waste = value_in_meal_plan * num_people - total_expected_consumption\n",
    "\n",
    "# Print results\n",
    "print(f\"Estimated number of people not eating the ingredient: {num_not_eating} out of {num_people}\")\n",
    "print(f\"Total expected consumption: {total_expected_consumption:.2f} grams\")\n",
    "print(f\"Total expected food waste for {num_people} people: {total_food_waste:.2f} grams\")\n",
    "\n",
    "# Optional: Print mean and standard deviation of the samples\n",
    "print(\"Mean of the normal distribution samples:\", np.mean(consumption_values))\n",
    "print(\"Standard deviation of the normal distribution samples:\", np.std(consumption_values))\n",
    "\n",
    "# Visualize the distribution\n",
    "plt.hist(consumption_values, bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Consumption Values')\n",
    "plt.xlabel('Consumption Value (grams)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "import gymnasium as gym\n",
    "\n",
    "register(\n",
    "    id='SchoolMealSelection-v1',\n",
    "    entry_point='models.envs.env_working:SchoolMealSelection',\n",
    "    max_episode_steps=10,  # Allow multiple steps per episode, adjust as needed\n",
    ")\n",
    "\n",
    "# Get the environment specification for 'SchoolMealSelection-v1'\n",
    "env_spec = gym.envs.registry.get('SchoolMealSelection-v1')\n",
    "\n",
    "if env_spec:\n",
    "    print(f\"\\nEnvironment ID: {env_spec.id}\")\n",
    "    print(f\"Entry Point: {env_spec.entry_point}\")\n",
    "    print(f\"Max Episode Steps: {env_spec.max_episode_steps}\")\n",
    "else:\n",
    "    print(\"Environment 'SchoolMealSelection-v1' is not registered.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(1000 // 8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_group_target_ranges = {\n",
    "    'fruit': 1, # 1 fruit a day per meal\n",
    "    'veg': 1, # 1 veg per day per meal\n",
    "    'non_processed_protein': 1, # Portion of non processed protein has to be provided accept if a portion of processed protein is provided. This current env is one day meal selection.\n",
    "    'processed_protein': 1, # Processed protein, see above ^\n",
    "    'carbs': 1, # Starchy food , a portion of this should be provided every day\n",
    "    'dairy': 1, # Dairy, a portion of this should be provided every day\n",
    "    'bread': 1, # Bread should be provided as well as a portion of starchy food\n",
    "    'confectionary': 0 # No confectionary should be provided\n",
    "}\n",
    "print(len(ingredient_group_target_ranges.keys()))\n",
    "food_group_counts = {k: 0 for k in ingredient_group_target_ranges.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "# Assuming the function and necessary variables are defined in a module named 'reward_module'\n",
    "from models.reward.reward import group_count_reward\n",
    "\n",
    "class TestGroupCountReward(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Setting up default values for the tests\n",
    "        self.ingredient_group_portion = {\n",
    "            'non_processed_protein': 300,\n",
    "            'processed_protein': 200,\n",
    "            'confectionary': 150,\n",
    "            'vegetables': 500,\n",
    "            'fruits': 300\n",
    "        }\n",
    "        \n",
    "        self.ingredient_group_count = {\n",
    "            'non_processed_protein': 3,\n",
    "            'processed_protein': 2,\n",
    "            'confectionary': 1,\n",
    "            'vegetables': 5,\n",
    "            'fruits': 3\n",
    "        }\n",
    "        \n",
    "        self.ingredient_group_portion_targets = {\n",
    "            'non_processed_protein': (100, 150),\n",
    "            'processed_protein': (80, 120),\n",
    "            'confectionary': (50, 60),\n",
    "            'vegetables': (80, 100),\n",
    "            'fruits': (90, 110)\n",
    "        }\n",
    "        \n",
    "        self.ingredient_group_count_targets = {\n",
    "            'non_processed_protein': 3,\n",
    "            'processed_protein': 2,\n",
    "            'confectionary': 1,\n",
    "            'vegetables': 5,\n",
    "            'fruits': 3\n",
    "        }\n",
    "\n",
    "    def test_all_targets_met(self):\n",
    "        ingredient_group_count_rewards = {group: 0 for group in self.ingredient_group_count}\n",
    "        \n",
    "        expected_rewards = {\n",
    "            'non_processed_protein': 50,\n",
    "            'processed_protein': 50,\n",
    "            'confectionary': 100,\n",
    "            'vegetables': 100,\n",
    "            'fruits': 100\n",
    "        }\n",
    "        \n",
    "        rewards, all_targets_met = group_count_reward(ingredient_group_count_rewards)\n",
    "        \n",
    "        self.assertEqual(rewards, expected_rewards)\n",
    "        self.assertTrue(all_targets_met)\n",
    "    \n",
    "    def test_protein_target_not_met(self):\n",
    "        self.ingredient_group_count['non_processed_protein'] = 2\n",
    "        \n",
    "        ingredient_group_count_rewards = {group: 0 for group in self.ingredient_group_count}\n",
    "        \n",
    "        expected_rewards = {\n",
    "            'non_processed_protein': -50,\n",
    "            'processed_protein': -50,\n",
    "            'confectionary': 100,\n",
    "            'vegetables': 100,\n",
    "            'fruits': 100\n",
    "        }\n",
    "        \n",
    "        rewards, all_targets_met = group_count_reward(ingredient_group_count_rewards)\n",
    "        \n",
    "        self.assertEqual(rewards, expected_rewards)\n",
    "        self.assertFalse(all_targets_met)\n",
    "    \n",
    "    def test_protein_target_exceeded(self):\n",
    "        self.ingredient_group_count['non_processed_protein'] = 4\n",
    "        \n",
    "        ingredient_group_count_rewards = {group: 0 for group in self.ingredient_group_count}\n",
    "        \n",
    "        expected_rewards = {\n",
    "            'non_processed_protein': -25,\n",
    "            'processed_protein': -25,\n",
    "            'confectionary': 100,\n",
    "            'vegetables': 100,\n",
    "            'fruits': 100\n",
    "        }\n",
    "        \n",
    "        rewards, all_targets_met = group_count_reward(ingredient_group_count_rewards)\n",
    "        \n",
    "        self.assertEqual(rewards, expected_rewards)\n",
    "        self.assertFalse(all_targets_met)\n",
    "    \n",
    "    def test_confectionary_target_not_met(self):\n",
    "        self.ingredient_group_count['confectionary'] = 0\n",
    "        \n",
    "        ingredient_group_count_rewards = {group: 0 for group in self.ingredient_group_count}\n",
    "        \n",
    "        expected_rewards = {\n",
    "            'non_processed_protein': 50,\n",
    "            'processed_protein': 50,\n",
    "            'confectionary': -100,\n",
    "            'vegetables': 100,\n",
    "            'fruits': 100\n",
    "        }\n",
    "        \n",
    "        rewards, all_targets_met = group_count_reward(ingredient_group_count_rewards)\n",
    "        \n",
    "        self.assertEqual(rewards, expected_rewards)\n",
    "        self.assertFalse(all_targets_met)\n",
    "    \n",
    "    def test_other_group_target_not_met(self):\n",
    "        self.ingredient_group_count['vegetables'] = 4\n",
    "        \n",
    "        ingredient_group_count_rewards = {group: 0 for group in self.ingredient_group_count}\n",
    "        \n",
    "        expected_rewards = {\n",
    "            'non_processed_protein': 50,\n",
    "            'processed_protein': 50,\n",
    "            'confectionary': 100,\n",
    "            'vegetables': -100,\n",
    "            'fruits': 100\n",
    "        }\n",
    "        \n",
    "        rewards, all_targets_met = group_count_reward(ingredient_group_count_rewards)\n",
    "        \n",
    "        self.assertEqual(rewards, expected_rewards)\n",
    "        self.assertFalse(all_targets_met)\n",
    "    \n",
    "    def test_other_group_target_exceeded(self):\n",
    "        self.ingredient_group_count['vegetables'] = 6\n",
    "        \n",
    "        ingredient_group_count_rewards = {group: 0 for group in self.ingredient_group_count}\n",
    "        \n",
    "        expected_rewards = {\n",
    "            'non_processed_protein': 50,\n",
    "            'processed_protein': 50,\n",
    "            'confectionary': 100,\n",
    "            'vegetables': 0,\n",
    "            'fruits': 100\n",
    "        }\n",
    "        \n",
    "        rewards, all_targets_met = group_count_reward(ingredient_group_count_rewards)\n",
    "        \n",
    "        self.assertEqual(rewards, expected_rewards)\n",
    "        self.assertFalse(all_targets_met)\n",
    "\n",
    "    def test_combination_of_targets(self):\n",
    "        self.ingredient_group_count['non_processed_protein'] = 4\n",
    "        self.ingredient_group_count['processed_protein'] = 3\n",
    "        self.ingredient_group_count['confectionary'] = 2\n",
    "        self.ingredient_group_count['vegetables'] = 4\n",
    "        self.ingredient_group_count['fruits'] = 2\n",
    "        \n",
    "        ingredient_group_count_rewards = {group: 0 for group in self.ingredient_group_count}\n",
    "        \n",
    "        expected_rewards = {\n",
    "            'non_processed_protein': -50,\n",
    "            'processed_protein': -50,\n",
    "            'confectionary': -100,\n",
    "            'vegetables': -100,\n",
    "            'fruits': -100\n",
    "        }\n",
    "        \n",
    "        rewards, all_targets_met = group_count_reward(ingredient_group_count_rewards)\n",
    "        \n",
    "        self.assertEqual(rewards, expected_rewards)\n",
    "        self.assertFalse(all_targets_met)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_line_size(file_path, num_lines=100):\n",
    "    with open(file_path, 'r') as file:\n",
    "        total_size = 0\n",
    "        for _ in range(num_lines):\n",
    "            line = file.readline()\n",
    "            total_size += len(line)\n",
    "    return total_size / num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.process_data import get_data\n",
    "ingredient_df = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ingredient_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Actor and Critic\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, selection_action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, selection_action_dim)\n",
    "        self.fc3 = nn.Linear(128, selection_action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        \n",
    "        quantity = torch.relu(self.fc3(x))\n",
    "        return quantity\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        value = self.fc2(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "x = os.path.abspath(\"Masters_RL/saved_models/tensorboard/a2c_simple_calorie_env/\")\n",
    "print(os.path.exists(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, ingredient_df, num_people=1):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        \n",
    "        self.ingredient_df = ingredient_df\n",
    "        self.num_people = num_people\n",
    "        \n",
    "        # Define action space\n",
    "        n_ingredients = len(self.ingredient_df)\n",
    "        \n",
    "        self.action_space = spaces.Dict({\n",
    "            'selection': spaces.MultiBinary(n_ingredients),\n",
    "            'quantity': spaces.Box(low=0, high=100, shape=(n_ingredients,), dtype=np.float32)\n",
    "        })\n",
    "        \n",
    "        # State includes quantities of ingredients and average calories\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(n_ingredients + 1,), dtype=np.float32)\n",
    "        \n",
    "        # self.state = None\n",
    "\n",
    "    # def reset(self):\n",
    "    #     n_ingredients = len(self.ingredient_df)\n",
    "    #     self.state = np.zeros(n_ingredients + 1, dtype=np.float32)  # Reset state to zeros including average calories\n",
    "    #     return self.state\n",
    "\n",
    "    def calculate_reward(self, action):\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Convert action selection and quantity to numpy arrays\n",
    "        selection = np.array(action['selection'], dtype=np.float32)\n",
    "        quantity = np.array(action['quantity'], dtype=np.float32)\n",
    "        \n",
    "        # Reward based on the number of selected ingredients\n",
    "        total_selection = np.sum(selection)\n",
    "        \n",
    "        if total_selection > 10:\n",
    "            reward -= 10\n",
    "        elif total_selection < 5:\n",
    "            reward -= 10\n",
    "        else:\n",
    "            reward += 10\n",
    "        \n",
    "        # Calculate calories for selected ingredients\n",
    "        calories_per_100g = self.ingredient_df['Calories_kcal_per_100g'].values\n",
    "        calories_selected_ingredients = selection * quantity * calories_per_100g / 100\n",
    "        \n",
    "        # Calculate average calories per day per person\n",
    "        average_calories_per_day = np.sum(calories_selected_ingredients) / self.num_people\n",
    "        \n",
    "        # Reward based on the average calories per day\n",
    "        if 2000 <= average_calories_per_day <= 3000:\n",
    "            reward += 100\n",
    "            done = True\n",
    "        else:\n",
    "            reward -= 10\n",
    "            done = False\n",
    "        \n",
    "        return reward, average_calories_per_day, done\n",
    "\n",
    "    def step(self, action):\n",
    "        n_ingredients = len(self.ingredient_df)\n",
    "        \n",
    "        # Update the state based on action\n",
    "        self.state[:n_ingredients] = action['quantity']  # Update state with quantities of selected ingredients\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward, average_calories_per_day, done = self.calculate_reward(action)\n",
    "        \n",
    "        # Update the state with average calories\n",
    "        self.state[-1] = average_calories_per_day\n",
    "        \n",
    "        info = {\n",
    "            'average_calories_per_day': average_calories_per_day\n",
    "        }\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "def train_actor_critic(env, actor, critic, num_episodes=1000, gamma=0.99, actor_lr=1e-4, critic_lr=1e-3, device='cuda'):\n",
    "    # Move models to the GPU\n",
    "    actor.to(device)\n",
    "    critic.to(device)\n",
    "    \n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        episode_info = None\n",
    "\n",
    "        while not done:\n",
    "            selection_probs, quantity = actor(state)\n",
    "            selection = (selection_probs > 0.5).float()  # Binarize selection actions\n",
    "            action = {\n",
    "                'selection': selection.detach().cpu().numpy().squeeze(),\n",
    "                'quantity': quantity.detach().cpu().numpy().squeeze()\n",
    "            }\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "            reward = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "\n",
    "            # Save the info for the end of the episode\n",
    "            episode_info = info\n",
    "\n",
    "            # Update Critic\n",
    "            value = critic(state)\n",
    "            next_value = critic(next_state)\n",
    "            target = reward + (1 - done) * gamma * next_value\n",
    "            critic_loss = mse_loss(value, target.detach())\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # Update Actor\n",
    "            advantage = (target - value).detach()\n",
    "            actor_loss = -advantage * (selection_probs.mean() + quantity.mean())\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Debug prints\n",
    "            print(f\"Reward: {reward.item()}, Done: {done}\")\n",
    "\n",
    "        if episode_info is not None and 'average_calories_per_day' in episode_info:\n",
    "            average_calories_per_day = episode_info['average_calories_per_day']\n",
    "        else:\n",
    "            average_calories_per_day = float('nan')\n",
    "\n",
    "        print(f\"Episode: {episode+1}, Total Reward: {total_reward.item()}, Average Calories per Day: {average_calories_per_day}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment and models\n",
    "env = CustomEnv(ingredient_df)\n",
    "\n",
    "state_dim = env.state.shape[0]\n",
    "selection_action_dim = env.action_space['selection'].n\n",
    "actor = Actor(state_dim, selection_action_dim)\n",
    "critic = Critic(state_dim)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_actor_critic(env, actor, critic, num_episodes=5)\n",
    "\n",
    "# Evaluate the model\n",
    "# evaluate_actor(env, actor, num_days=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_actor(env, actor, num_days=5):\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_selections = []\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            selection_probs, quantity = actor(state)\n",
    "            selection = (selection_probs > 0.5).float()  # Binarize selection actions\n",
    "            action = {\n",
    "                'selection': selection.detach().numpy().squeeze(),\n",
    "                'quantity': quantity.detach().numpy().squeeze()\n",
    "            }\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        episode_selections.append(action)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Evaluation Total Reward: {total_reward}\")\n",
    "    for day, selections in enumerate(env.selections, 1):\n",
    "        print(f\"Day {day}:\")\n",
    "        for i, (selection, quantity) in enumerate(selections):\n",
    "            print(f\"  Try {i+1}: Selection - {selection}, Quantity - {quantity}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
