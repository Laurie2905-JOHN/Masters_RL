{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.process_data import get_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 137 lines from the file. Loaded 136 ingredients.\n"
     ]
    }
   ],
   "source": [
    "ingredient_df = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Category1', 'Category2', 'Category3', 'Category4', 'Category5',\n",
      "       'Category6', 'Category7', 'Mean_g_per_day', 'StandardDeviation',\n",
      "       'Coefficient of Variation', 'Cost_100g', 'Calories_kcal_per_100g',\n",
      "       'Fat_g', 'Saturates_g', 'Carbs_g', 'Sugars_g', 'Fibre_g', 'Protein_g',\n",
      "       'Salt_g', 'CO2_kg_per_100g', 'Animal Welfare Rating',\n",
      "       'Rainforest Rating', 'Water Scarcity Rating', 'CO2 FU Rating',\n",
      "       'Group A veg', 'Group A fruit', 'Group B', 'Oily Fish', 'Red Meat',\n",
      "       'Group C', 'Group D', 'Group E', 'Oil', 'Bread', 'Confectionary'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(ingredient_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, ingredient_df, num_days=5, num_people=50):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        \n",
    "        # Number of ingredients\n",
    "        self.ingredient_df = ingredient_df\n",
    "        self.num_ingredients = len(ingredient_df)\n",
    "        self.num_days = num_days\n",
    "        self.num_people = num_people\n",
    "        \n",
    "        # Action space: ingredient amount (between 0 and 100 per ingredient)\n",
    "        self.action_space = spaces.Box(low=np.zeros(self.num_ingredients), high=np.ones(self.num_ingredients) * 100, dtype=np.float32)\n",
    "        \n",
    "        # Observation space: average calories per day per person\n",
    "        self.observation_space = spaces.Box(low=np.array([0.0]), high=np.array([10000.0]), dtype=np.float32)\n",
    "        \n",
    "        # Initial state: average calories per day per person\n",
    "        self.state = np.zeros(1, dtype=np.float32)\n",
    "        \n",
    "        # Set day count\n",
    "        self.day_count = 0\n",
    "\n",
    "    def step(self, action):\n",
    "            # Ensure the action is within the action space\n",
    "            action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "            \n",
    "            # Calculate total calories for the chosen ingredient amounts\n",
    "            total_calories = np.sum(action * self.ingredient_df['Calories_kcal_per_100g'].values)\n",
    "            \n",
    "            # Calculate average calories per day per person\n",
    "            average_calories_per_day = total_calories / (self.num_days * self.num_people)\n",
    "            \n",
    "            # Update state\n",
    "            self.state = np.array([average_calories_per_day], dtype=np.float32)\n",
    "            \n",
    "            # Calculate reward: reward is 1 if average calories are between 2000 and 3000, else -1\n",
    "            if 2000 <= average_calories_per_day <= 3000:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "            \n",
    "            # Increment day count\n",
    "            self.day_count += 1\n",
    "            \n",
    "            # Check if episode is done (after the specified number of days)\n",
    "            done = self.day_count >= self.num_days\n",
    "            \n",
    "            # Placeholder for info\n",
    "            info = {\n",
    "                'total_calories': total_calories,\n",
    "                'average_calories_per_day': average_calories_per_day\n",
    "            }\n",
    "            \n",
    "            # Return step information\n",
    "            return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state to zero average calories\n",
    "        self.state = np.zeros(1, dtype=np.float32)\n",
    "        # Reset day count\n",
    "        self.day_count = 0\n",
    "        return self.state\n",
    "\n",
    "    def render(self):\n",
    "        # Implement visualization (optional)\n",
    "        print(f\"Day: {self.day_count}, Average Calories per Day: {self.state[0]}\")\n",
    "\n",
    "    def close(self):\n",
    "        # Clean up (optional)\n",
    "        pass\n",
    "\n",
    "# Instantiate the environment\n",
    "env = CustomEnv(ingredient_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: -5.0, Steps: 5, Calories: 5100.39990234375\n",
      "Episode: 2, Total Reward: -5.0, Steps: 5, Calories: 5007.2001953125\n",
      "Episode: 3, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 4, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 5, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 6, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 7, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 8, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 9, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 10, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 11, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 12, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 13, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 14, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 15, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 16, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 17, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 18, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 19, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 20, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 21, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 22, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 23, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 24, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 25, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 26, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 27, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 28, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 29, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 30, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 31, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 32, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 33, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 34, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 35, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 36, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 37, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 38, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 39, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 40, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 41, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 42, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 43, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 44, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 45, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 46, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 47, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 48, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 49, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "Episode: 50, Total Reward: -5.0, Steps: 5, Calories: 4890.7998046875\n",
      "[79.18291   94.27029   49.378708  43.850315  50.81982   84.17008\n",
      " 89.57092   50.218525  82.27186   42.37283   98.446304  26.437471\n",
      " 29.766655  79.986145   8.814528  91.54294    8.850589  15.926081\n",
      " 34.76103   91.75782   27.247784  34.569595  64.23192   50.010704\n",
      " 84.00745   93.50856   31.33487   32.42415   52.579803  65.54815\n",
      " 48.20764   22.351671  29.6561    51.93661   78.48376    0.8657546\n",
      " 93.31107   56.90956   10.5580845 38.87606   27.229185  46.829136\n",
      " 35.011635  35.141262  52.355553  72.11723   27.835207  31.796375\n",
      " 88.80209    2.631663  31.743555  21.048061  36.155495  54.598896\n",
      " 85.3991    11.822082  76.30656   56.084015  75.419075  39.103806\n",
      " 20.783737  47.181644  52.822792  15.752074  42.85098    2.9919972\n",
      "  1.9520618  7.8393016 64.97188   47.473343  22.776735  50.829113\n",
      " 92.162674  11.800118  37.615135  85.4558    97.49982   80.38385\n",
      " 26.238087  55.687782  53.919197  49.006893  94.157616  47.268017\n",
      " 28.583952  50.78439   58.9313    37.8554    95.63914   32.483562\n",
      " 15.701571  47.63851   68.57193   62.671192  71.76994   73.91065\n",
      " 88.74818   13.761038  12.178534  25.682384  97.89315   71.69583\n",
      " 81.77494   47.00574   97.25006   77.30566   55.170307   3.8021696\n",
      " 89.057304  97.47025   25.101788   7.7076635 44.892456   8.8130245\n",
      " 76.47898   83.528275  18.088493  26.26141   14.947697  32.18438\n",
      " 14.960603  57.46885   43.07715   22.98117   49.60713    8.083124\n",
      " 62.587955   1.4333022 52.25948   59.359875  43.314445  62.764973\n",
      " 77.83647   51.02802   52.06726   66.348785 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        return x * 100  # Ensure actions are in the range 0 to 100\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def train_actor_critic(env, actor, critic, num_episodes=100000, gamma=0.99, actor_lr=1e-4, critic_lr=1e-3):\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            action = actor(state)\n",
    "            action = action.detach().numpy()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "            reward = torch.tensor(reward, dtype=torch.float32)\n",
    "\n",
    "            # Update Critic\n",
    "            value = critic(state)\n",
    "            next_value = critic(next_state)\n",
    "            target = reward + (1 - done) * gamma * next_value\n",
    "            critic_loss = mse_loss(value, target.detach())\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # Update Actor\n",
    "            advantage = (reward + (1 - done) * gamma * next_value - value).detach()\n",
    "            actor_loss = -advantage * torch.sum(actor(state) * torch.FloatTensor(action))\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward.item()\n",
    "            step_count += 1\n",
    "\n",
    "        print(f\"Episode: {episode+1}, Total Reward: {total_reward}, Steps: {step_count}, Calories: {env.state[0]}\")\n",
    "\n",
    "# Instantiate the environment\n",
    "env = CustomEnv(ingredient_df)\n",
    "\n",
    "# Instantiate the actor and critic networks\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim)\n",
    "\n",
    "# Train the actor-critic model\n",
    "train_actor_critic(env, actor, critic, num_episodes=50)\n",
    "\n",
    "print(env.action_space.sample())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m critic \u001b[38;5;241m=\u001b[39m Critic(state_dim)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Train the actor-critic model\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mtrain_actor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 16\u001b[0m, in \u001b[0;36mtrain_actor_critic\u001b[0;34m(env, actor, critic, num_episodes, gamma, actor_lr, critic_lr)\u001b[0m\n\u001b[1;32m     14\u001b[0m action \u001b[38;5;241m=\u001b[39m actor(state)\n\u001b[1;32m     15\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 16\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(next_state)\n\u001b[1;32m     18\u001b[0m reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(reward, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[0;32mIn[45], line 37\u001b[0m, in \u001b[0;36mCustomEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate total calories for the chosen ingredient amounts\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m total_calories \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mingredient_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCalories_kg_per_100g\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Calculate average calories per day per person\u001b[39;00m\n\u001b[1;32m     40\u001b[0m average_calories_per_day \u001b[38;5;241m=\u001b[39m total_calories \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_days \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_people)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "def train_actor_critic(env, actor, critic, num_episodes=1000, gamma=0.99, actor_lr=1e-4, critic_lr=1e-3):\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            action = actor(state)\n",
    "            action = action.detach().numpy()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "            reward = torch.tensor(reward, dtype=torch.float32)\n",
    "\n",
    "            # Update Critic\n",
    "            value = critic(state)\n",
    "            next_value = critic(next_state)\n",
    "            target = reward + (1 - done) * gamma * next_value\n",
    "            critic_loss = mse_loss(value, target.detach())\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # Update Actor\n",
    "            advantage = (reward + (1 - done) * gamma * next_value - value).detach()\n",
    "            actor_loss = -advantage * torch.sum(actor(state) * torch.FloatTensor(action))\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward.item()\n",
    "            step_count += 1\n",
    "\n",
    "        print(f\"Episode: {episode+1}, Total Reward: {total_reward}, Steps: {step_count}\")\n",
    "\n",
    "# Instantiate the environment\n",
    "env = CustomEnv(ingredient_df)\n",
    "\n",
    "# Instantiate the actor and critic networks\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim)\n",
    "\n",
    "# Train the actor-critic model\n",
    "train_actor_critic(env, actor, critic, num_episodes=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
